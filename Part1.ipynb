{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Part 1** of the project is divided into three sections:\n",
    "\n",
    "1- Feature Extraction (Using SIFT)\n",
    "\n",
    "2- Outlier Removal (Using RANSAC)\n",
    "\n",
    "3- Computing the Homographies (Using DLT)\n",
    "\n",
    "\n",
    "**pip install opencv-python**\n",
    "\n",
    "**pip install opencv-contrib-python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from numpy.linalg import eig\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from src.extract_features import *\n",
    "from src.matching_features import *\n",
    "from src.homography import *\n",
    "from src.ransac import *\n",
    "from src.parsing import *\n",
    "from src.display_video import *\n",
    "from main import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image matches:  [('225', '131'), ('580', '120'), ('626', '305'), ('133', '303')]\n",
      "map matches:  [('225', '131'), ('580', '120'), ('626', '305'), ('133', '303')] \n",
      "\n",
      "[[ 1.00000000e+00 -1.83423599e-13  1.22165870e-11]\n",
      " [ 8.31845281e-14  1.00000000e+00  7.08329962e-12]\n",
      " [ 2.88444403e-16 -9.61481343e-16  1.00000000e+00]]\n",
      "Condition:  1.0000000000141258 \n",
      "\n",
      "Total frames of the video:  1901\n",
      "(Nº features, Nº descriptors per feature):  (5000, 128)\n",
      "Nº of frames extracted:  20\n",
      "H_sequential [[ 1.00000000e+00  2.00000000e+00  3.00000000e+00  4.00000000e+00\n",
      "   5.00000000e+00  6.00000000e+00  7.00000000e+00  8.00000000e+00\n",
      "   9.00000000e+00  1.00000000e+01  1.10000000e+01  1.20000000e+01\n",
      "   1.30000000e+01  1.40000000e+01  1.50000000e+01  1.60000000e+01\n",
      "   1.70000000e+01  1.80000000e+01  1.90000000e+01]\n",
      " [ 2.00000000e+00  3.00000000e+00  4.00000000e+00  5.00000000e+00\n",
      "   6.00000000e+00  7.00000000e+00  8.00000000e+00  9.00000000e+00\n",
      "   1.00000000e+01  1.10000000e+01  1.20000000e+01  1.30000000e+01\n",
      "   1.40000000e+01  1.50000000e+01  1.60000000e+01  1.70000000e+01\n",
      "   1.80000000e+01  1.90000000e+01  2.00000000e+01]\n",
      " [ 1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  1.00000000e+00  1.00000000e+00\n",
      "   1.00000000e+00  1.00000000e+00  1.00000000e+00]\n",
      " [ 1.27562654e-01  1.32362532e-01  9.51598796e-02  1.17968403e-01\n",
      "   1.19044551e-01  4.92057087e-01  1.91442721e-01  1.54788617e-01\n",
      "   1.35108852e-01  1.04450659e-01  1.10119132e-01  1.55914161e-01\n",
      "   1.66816347e-01  1.13709491e-01  1.16448694e-01  1.41693246e-01\n",
      "  -9.78105605e-01  9.61875143e-02  1.89559137e-01]\n",
      " [-4.24552153e+01 -4.29298725e+01 -3.41741167e+01 -4.20197832e+01\n",
      "  -4.25239571e+01  1.33560550e+02 -4.59895016e+01 -4.58300552e+01\n",
      "  -5.90281414e+01 -5.47475750e+01 -5.01340771e+01 -4.73304066e+01\n",
      "  -4.83666287e+01 -3.97089365e+01 -3.87787367e+01 -4.85222741e+01\n",
      "   6.46347794e+02 -3.29240647e+01 -5.69757332e+01]\n",
      " [ 6.23992643e-03  7.99909886e-03  1.81308569e-03  7.68063113e-03\n",
      "   5.54984864e-03 -3.56214387e-02  6.83890405e-03  7.79478822e-03\n",
      "   8.70951417e-03  1.00371404e-02  8.29401851e-03  5.16619880e-03\n",
      "   3.07272021e-03  7.61557615e-03  3.51509364e-03  7.62392572e-03\n",
      "  -3.59452702e-01  9.05446100e-03  1.28726991e-02]\n",
      " [ 9.84995451e-01  9.74440860e-01  9.73808354e-01  9.84255841e-01\n",
      "   9.77922888e-01  1.45681715e+00  1.00426787e+00  9.86411162e-01\n",
      "   9.85092374e-01  9.76251748e-01  9.80605870e-01  9.99201823e-01\n",
      "   1.00409243e+00  9.89863510e-01  9.83588908e-01  9.83968469e-01\n",
      "   1.80873870e+00  9.68311598e-01  9.82956897e-01]\n",
      " [-3.23418795e+00 -2.39906063e+00  1.10559250e+00 -5.11714058e+00\n",
      "  -2.89752421e+00 -4.12026382e+01 -5.60424083e+00 -2.84903297e+00\n",
      "  -5.81110521e+00 -5.47710623e+00 -4.62768527e+00 -4.62911720e+00\n",
      "  -5.08907445e+00 -5.14131121e+00 -1.71438271e+00 -5.86650554e+00\n",
      "   1.09341905e+02 -2.31556771e+00 -7.75684164e+00]\n",
      " [ 8.26699385e-05  1.00374515e-04  6.74333963e-05  1.00291427e-04\n",
      "   8.88098409e-05 -8.60355588e-04  8.66600508e-05  1.02934398e-04\n",
      "   1.02947535e-04  1.06026514e-04  8.98222379e-05  7.06613805e-05\n",
      "   7.06363699e-05  9.72873215e-05  7.81146428e-05  9.71324380e-05\n",
      "  -2.86001568e-03  8.77755399e-05  1.22533725e-04]\n",
      " [ 3.27613507e-05 -1.52870451e-05 -5.70159802e-05  2.71797273e-05\n",
      "  -5.55988464e-06  2.50289762e-03  1.10871212e-04 -8.55264525e-06\n",
      "   9.14823145e-05  5.91472140e-05  4.01524302e-05  1.08769840e-04\n",
      "   1.10588552e-04  2.94419570e-05  1.80484208e-05  7.52097779e-05\n",
      "   9.84649263e-04 -1.29063889e-05  9.66009009e-05]\n",
      " [ 9.42215259e-01  9.43644521e-01  9.69886260e-01  9.31315622e-01\n",
      "   9.43775773e-01  9.65533316e-01  9.28249659e-01  9.47597463e-01\n",
      "   9.11637731e-01  9.12649742e-01  9.31396348e-01  9.37082370e-01\n",
      "   9.34707909e-01  9.35821518e-01  9.49891395e-01  9.18305943e-01\n",
      "   2.49526310e+00  9.37332931e-01  8.99640770e-01]]\n",
      "H_output [[ 0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [ 1.00000000e+00  2.00000000e+00  3.00000000e+00  4.00000000e+00\n",
      "   5.00000000e+00  6.00000000e+00  7.00000000e+00  8.00000000e+00\n",
      "   9.00000000e+00  1.00000000e+01  1.10000000e+01  1.20000000e+01\n",
      "   1.30000000e+01  1.40000000e+01  1.50000000e+01  1.60000000e+01\n",
      "   1.70000000e+01  1.80000000e+01  1.90000000e+01]\n",
      " [ 1.00000000e+00  1.00000000e+00  9.96758965e-01  9.91608394e-01\n",
      "   9.82807673e-01  9.72016834e-01  1.11087295e+00  1.11038415e+00\n",
      "   1.10593888e+00  1.09894827e+00  1.08812781e+00  1.07499264e+00\n",
      "   1.06021125e+00  1.04008793e+00  1.01537686e+00  9.89327118e-01\n",
      "   9.61430604e-01  1.56330150e+00  1.55602547e+00]\n",
      " [-1.83423599e-13  1.27562654e-01  2.57313809e-01  3.50175147e-01\n",
      "   4.58526439e-01  5.66236264e-01  8.40560294e-01  1.04883635e+00\n",
      "   1.20750734e+00  1.32337079e+00  1.39328280e+00  1.47504755e+00\n",
      "   1.60699348e+00  1.75119470e+00  1.84019735e+00  1.92072367e+00\n",
      "   1.99717374e+00  2.21761624e+00  2.30173590e+00]\n",
      " [ 1.22165870e-11 -4.24552153e+01 -8.32985344e+01 -1.14568977e+02\n",
      "  -1.50158943e+02 -1.84837835e+02 -7.19744130e+01 -1.22609420e+02\n",
      "  -1.70061512e+02 -2.27332960e+02 -2.74888362e+02 -3.17029975e+02\n",
      "  -3.54791207e+02 -3.91083100e+02 -4.16288203e+02 -4.37958416e+02\n",
      "  -4.61452154e+02 -3.11651201e+02 -3.48726214e+02]\n",
      " [ 8.31845281e-14  6.23992643e-03  1.37943724e-02  1.51529801e-02\n",
      "   2.18574998e-02  2.60964775e-02  5.04744741e-03  9.91467602e-03\n",
      "   1.48485290e-02  2.06802725e-02  2.74420550e-02  3.23625012e-02\n",
      "   3.42239634e-02  3.33183625e-02  3.58377172e-02  3.44324023e-02\n",
      "   3.64759935e-02 -1.68538479e-01 -1.53563474e-01]\n",
      " [ 1.00000000e+00  9.84995451e-01  9.60695188e-01  9.37169684e-01\n",
      "   9.24068557e-01  9.06325492e-01  1.30050554e+00  1.30187041e+00\n",
      "   1.28614740e+00  1.26420824e+00  1.23303904e+00  1.20977514e+00\n",
      "   1.20710007e+00  1.21052439e+00  1.20001508e+00  1.18319547e+00\n",
      "   1.16370357e+00  1.99572938e+00  1.91673173e+00]\n",
      " [ 7.08329962e-12 -3.23418795e+00 -5.68286679e+00 -4.92100752e+00\n",
      "  -1.00153651e+01 -1.30592374e+01 -4.64666702e+01 -5.06531467e+01\n",
      "  -5.21622551e+01 -5.59034989e+01 -5.90767115e+01 -6.21057320e+01\n",
      "  -6.53301078e+01 -6.88628884e+01 -7.19900921e+01 -7.18297955e+01\n",
      "  -7.45736894e+01 -3.52632318e+01 -3.21256632e+01]\n",
      " [ 2.88444403e-16  8.26699385e-05  1.77506399e-04  2.37269512e-04\n",
      "   3.22751752e-04  3.92654999e-04 -2.35036132e-04 -1.55462257e-04\n",
      "  -6.69305858e-05  1.94230366e-05  1.03746671e-04  1.68824099e-04\n",
      "   2.13472170e-04  2.50547336e-04  3.04896768e-04  3.39068570e-04\n",
      "   3.85901913e-04 -1.15957616e-03 -1.02416702e-03]\n",
      " [-9.61481343e-16  3.27613507e-05  2.84627139e-05 -5.87828771e-06\n",
      "   4.53831289e-05  7.84425570e-05  2.12539969e-03  2.17268364e-03\n",
      "   2.11314681e-03  2.13298357e-03  2.11945478e-03  2.11100718e-03\n",
      "   2.18758401e-03  2.27966847e-03  2.29624402e-03  2.30009546e-03\n",
      "   2.33395029e-03  4.08729944e-03  3.83177398e-03]\n",
      " [ 1.00000000e+00  9.42215259e-01  8.85488660e-01  8.52788629e-01\n",
      "   7.84275439e-01  7.26323978e-01  7.50501177e-01  6.95550404e-01\n",
      "   6.60036595e-01  5.93385334e-01  5.28807030e-01  4.77519523e-01\n",
      "   4.29712514e-01  3.80197978e-01  3.34127996e-01  3.01625156e-01\n",
      "   2.47038272e-01  1.12105091e+00  1.07951147e+00]]\n"
     ]
    }
   ],
   "source": [
    "config_data = parse_configuration_file('config/part_1.cfg') #Parse the configuration file\n",
    "match_img1 , match_map = parse_points(config_data) #Parse the points from the configuration file\n",
    "video_path = config_data[0].split(' ')[1].strip() #Get the video path\n",
    "H_frame1_to_map =compute_homography(match_img1, match_map)\n",
    "print(H_frame1_to_map)\n",
    "print(\"Condition: \", np.linalg.cond(H_frame1_to_map), '\\n')\n",
    "\n",
    "sift_points, kp_list = extract_features(video_path)\n",
    "#homography_two_frames(img1, img2, sift_points, kp_list, 1) #option 1 - with openCV; option 2 - with numpy\n",
    "\n",
    "match = matching_features_SCIKITLEARN(sift_points)\n",
    "# print(match2)\n",
    "\n",
    "H_sequential = create_sequential_homographies(match, sift_points)\n",
    "print('H_sequential' , H_sequential)\n",
    "H_output = homography_to_map(H_sequential, H_frame1_to_map)\n",
    "print('H_output', H_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "H_output=np.empty([11,0])\n",
    "H_i = np.vstack((np.array([[0], [1]]) , H_frame1_to_map.reshape(9,1) )) #first part of the array is 0 and 1 - which means homography from frame 1 to map (frame 0)\n",
    "H_output = np.hstack([H_output, H_i])\n",
    "                        \n",
    "for i in range(1, len(H_sequential)):\n",
    "    T_to_map= np.matmul(  H_output[2:,i-1].reshape(3,3), H_sequential[2:,i-1].reshape(3,3)) \n",
    "    #for frame n, H_output[2:,i-1] should be the homography from frame n-1 to the map. \n",
    "    # H_sequential[2:,i-1] should be the homography from frame n to n-1\n",
    "    # So T_to_Map should be the homography from frame n to map\n",
    "\n",
    "    H_i = np.vstack(( np.array([[0],[H_sequential[1,i-1]]] ), T_to_map.reshape(9,1) ))\n",
    "\n",
    "    H_output = np.hstack([H_output, H_i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_features_frames(video_path= 'video/trymefirst_lisbon.mp4'):\n",
    "    \"\"\"Extracts the features from the video and stores them in a list\"\"\"\n",
    "    print(video_path)\n",
    "    capture = cv2.VideoCapture(os.path.abspath(video_path))\n",
    "    kp_list = []\n",
    "    sift_points = [] #nome a definir no config\n",
    "    sift = cv2.SIFT_create(5000) #number of sift points\n",
    "    img1, img2 = None, None\n",
    "    k = 0\n",
    "    frames=[]\n",
    "    count_frames(video_path)\n",
    "    while k <= 1900:\n",
    "        capture.set(cv2.CAP_PROP_POS_FRAMES, k)\n",
    "        success, frame = capture.read() #read the video\n",
    "        if success:\n",
    "            if (k == 0):\n",
    "                img1 = frame\n",
    "            if (k == 1900):\n",
    "                img2 = frame\n",
    "            frame_points = []\n",
    "            frames.append(frame)\n",
    "            gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY) #convert image to gray\n",
    "            key_points, descriptors = sift.detectAndCompute(gray,None) \n",
    "            kp_list.append(key_points)\n",
    "            frame_points = ([key_points[0].pt[0],key_points[0].pt[1]]+descriptors[0].tolist())\n",
    "            for i in range(1,len(key_points)):\n",
    "                 temp_column = ([key_points[i].pt[0],key_points[i].pt[1]]+descriptors[i].tolist())\n",
    "                 frame_points = np.column_stack((frame_points,temp_column))  \n",
    "        sift_points.append(frame_points) #append everything into a list \n",
    "        k += 100\n",
    "    print(\"(Nº features, Nº descriptors per feature): \", descriptors.shape)\n",
    "    print(\"Nº of frames extracted: \", len(sift_points))\n",
    "    return frames\n",
    "    \n",
    "def show_pixel_value(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        # Get the BGR values at the clicked position\n",
    "        b, g, r = img[y, x]\n",
    "        print(f\"Pixel value at (x={x}, y={y}): B={b}, G={g}, R={r}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def display(frame1, frame2,homography_de2_para1 ):\n",
    "    \n",
    "\n",
    "    # Display frame 1\n",
    "    H= homography_de2_para1.reshape((3,3))\n",
    "    \n",
    "\n",
    "    height, width = frame2.shape[:2]\n",
    "    warped_frame2 = cv2.warpPerspective(frame2, H, (width, height))\n",
    "\n",
    "    # Apply homography to frame 2\n",
    "\n",
    "    # Display frame 2 with homography applied\n",
    "    while True:\n",
    "        cv2.namedWindow('Image')\n",
    "        cv2.setMouseCallback('Image', show_pixel_value)\n",
    "        cv2.imshow(\"Frame 1\", frame1)\n",
    "        cv2.namedWindow('Image2')\n",
    "        cv2.setMouseCallback('Image2', show_pixel_value)\n",
    "        cv2.imshow(\"Frame 2 with Homography\", warped_frame2)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video/trymefirst_lisbon.mp4\n",
      "Total frames of the video:  1901\n",
      "(Nº features, Nº descriptors per feature):  (5000, 128)\n",
      "Nº of frames extracted:  20\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video/trymefirst_lisbon.mp4\n",
      "Total frames of the video:  1901\n",
      "(Nº features, Nº descriptors per feature):  (5000, 128)\n",
      "Nº of frames extracted:  20\n"
     ]
    }
   ],
   "source": [
    "frames= extract_features_frames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 11 is out of bounds for axis 1 with size 11",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;28mlen\u001b[39m(frames)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m      4\u001b[0m     img2\u001b[38;5;241m=\u001b[39m frames[i]\n\u001b[0;32m----> 5\u001b[0m     display(img1, img2, \u001b[43mH_output\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m )\n",
      "\u001b[0;31mIndexError\u001b[0m: index 11 is out of bounds for axis 1 with size 11"
     ]
    }
   ],
   "source": [
    "#to test homography\n",
    "img1=frames[0]\n",
    "for i in range(1,len(frames)-1):\n",
    "    img2= frames[i]\n",
    "    display(img1, img2, H_output[2:,i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alexa\\OneDrive - Universidade de Lisboa\\4º Ano\\1º Semestre - MEEC\\PIV\\Project\\Git\\Image-Processing-and-Vision---Project\\Part1.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmain\u001b[39;00m \u001b[39mimport\u001b[39;00m\u001b[39m*\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m H_output \u001b[39m=\u001b[39m homography_to_map(H_sequential, H_frame1_to_map)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mH_output\u001b[39m\u001b[39m'\u001b[39m, H_output)\n",
      "File \u001b[1;32mc:\\Users\\alexa\\OneDrive - Universidade de Lisboa\\4º Ano\\1º Semestre - MEEC\\PIV\\Project\\Git\\Image-Processing-and-Vision---Project\\main.py:54\u001b[0m, in \u001b[0;36mhomography_to_map\u001b[1;34m(H_sequential, H_frame1_to_map)\u001b[0m\n\u001b[0;32m     51\u001b[0m H_i \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack((np\u001b[39m.\u001b[39marray([[\u001b[39m0\u001b[39m], [\u001b[39m1\u001b[39m]]) , H_frame1_to_map\u001b[39m.\u001b[39mreshape(\u001b[39m9\u001b[39m,\u001b[39m1\u001b[39m) )) \u001b[39m#first part of the array is 0 and 1 - which means homography from frame 1 to map (frame 0)\u001b[39;00m\n\u001b[0;32m     52\u001b[0m H_output \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mhstack([H_output, H_i])\n\u001b[1;32m---> 54\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, H_sequential\u001b[39m.\u001b[39;49mshape(\u001b[39m1\u001b[39;49m)):\n\u001b[0;32m     55\u001b[0m     T_to_map\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(  H_output[\u001b[39m2\u001b[39m:,i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mreshape(\u001b[39m3\u001b[39m,\u001b[39m3\u001b[39m), H_sequential[\u001b[39m2\u001b[39m:,i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mreshape(\u001b[39m3\u001b[39m,\u001b[39m3\u001b[39m)) \n\u001b[0;32m     56\u001b[0m     \u001b[39m#for frame n, H_output[2:,i-1] should be the homography from frame n-1 to the map. \u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[39m# H_sequential[2:,i-1] should be the homography from frame n to n-1\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     \u001b[39m# So T_to_Map should be the homography from frame n to map\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "from main import*\n",
    "H_output = homography_to_map(H_sequential, H_frame1_to_map)\n",
    "print('H_output', H_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_frames(video_path):\n",
    "    \"\"\"Displays the video and counts the number of frames\"\"\"\n",
    "    capture = cv2.VideoCapture(os.path.abspath(video_path))\n",
    "    total_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"Total frames of the video: \", total_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code **extracts SIFT features** from each frame of the input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames of the video:  0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'frame_points' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m                  temp_column \u001b[38;5;241m=\u001b[39m ([key_points[i]\u001b[38;5;241m.\u001b[39mpt[\u001b[38;5;241m0\u001b[39m],key_points[i]\u001b[38;5;241m.\u001b[39mpt[\u001b[38;5;241m1\u001b[39m]]\u001b[38;5;241m+\u001b[39mdescriptors[i]\u001b[38;5;241m.\u001b[39mtolist())\n\u001b[1;32m     24\u001b[0m                  frame_points \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcolumn_stack((frame_points,temp_column))  \n\u001b[0;32m---> 25\u001b[0m         sift_points\u001b[38;5;241m.\u001b[39mappend(\u001b[43mframe_points\u001b[49m) \u001b[38;5;66;03m#append everything into a list \u001b[39;00m\n\u001b[1;32m     26\u001b[0m         k \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Nº features, Nº descriptors per feature): \u001b[39m\u001b[38;5;124m\"\u001b[39m, descriptors\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'frame_points' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "capture = cv2.VideoCapture(os.path.abspath('Video/trymefirst_lisbon.mp4'))\n",
    "kp_list = []\n",
    "sift_points = [] #nome a definir no config\n",
    "t = 0 \n",
    "sift = cv2.SIFT_create(5000) #number of sift points\n",
    "img1, img2 = None, None\n",
    "k = 0\n",
    "count_frames(os.path.abspath('Video/trymefirst_lisbon.mp4'))\n",
    "while k <= 1900:\n",
    "        capture.set(cv2.CAP_PROP_POS_FRAMES, k)\n",
    "        success, frame = capture.read() #read the video\n",
    "        if success:\n",
    "            if (k == 0):\n",
    "                img1 = frame\n",
    "            if (k == 1900):\n",
    "                img2 = frame\n",
    "            frame_points = []\n",
    "            gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY) #convert image to gray\n",
    "            key_points, descriptors = sift.detectAndCompute(gray,None) \n",
    "            kp_list.append(key_points)\n",
    "            frame_points = ([key_points[0].pt[0],key_points[0].pt[1]]+descriptors[0].tolist())\n",
    "            for i in range(1,len(key_points)):\n",
    "                 temp_column = ([key_points[i].pt[0],key_points[i].pt[1]]+descriptors[i].tolist())\n",
    "                 frame_points = np.column_stack((frame_points,temp_column))  \n",
    "        sift_points.append(frame_points) #append everything into a list \n",
    "        k += 100\n",
    "print(\"(Nº features, Nº descriptors per feature): \", descriptors.shape)\n",
    "print(\"Nº of frames extracted: \", len(sift_points))\n",
    "#print(des.shape)\n",
    "#print(len(sift_points))\n",
    "#The keypoint is a point of interest in the image, the descriptor is a vector that describes the image patch around the keypoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code **matches SIFT features** between the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Brute force method\n",
    "bf = cv2.BFMatcher(crossCheck=True) #crossCheck is set to true so that the match is symmetric\n",
    "all_matches = []\n",
    "match = []\n",
    "for s in range(len(sift_points)-1):\n",
    "    point_matches = []\n",
    "\n",
    "    des1 = (((sift_points[s])[2:,:])).astype('float32')  # descriptors of the first frame\n",
    "    des2 = (((sift_points[s+1])[2:,:])).astype('float32')  # descriptors of the second\n",
    "    des1 = np.reshape(des1,(np.shape(des1)[1],128))\n",
    "    des2 = np.reshape(des2,(np.shape(des2)[1],128))\n",
    "\n",
    "    if np.shape(des1)[0] > np.shape(des2)[0]:\n",
    "             des1 = des1[:-abs(np.shape(des1)[0]-np.shape(des2)[0]),:]  # we are removing the last points so that we have an equal amount of SIFT features between two frames\n",
    "    if np.shape(des1)[0] < np.shape(des2)[0]:\n",
    "             des2 = des2[:-abs(np.shape(des1)[0]-np.shape(des2)[0]),:]\n",
    "    matches = bf.match(des1,des2)  # an error occurs if two frames have different amounts of SIFT features\n",
    "\n",
    "    for i in range(len(matches)):\n",
    "        match.append(matches)\n",
    "        point_matches.append([matches[i].queryIdx,matches[i].trainIdx])\n",
    "\n",
    "    all_matches.append(point_matches)\n",
    "\n",
    "    \n",
    "#Feature detection: opencv\n",
    "#Matching : sklearn , numpy\n",
    "#RANSAC: numpy\n",
    "#Create Homography: numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Feature matching using nearest neighbours, for pairs of consecutive frames\"\"\"\n",
    "    \n",
    "matches=[]\n",
    "Threshold=0.75\n",
    "\n",
    "for s in range(len(sift_points)-1):\n",
    "    frame1_descriptors = sift_points[s][2:,:] #descriptor values of every feature point for video frame s (current shape: 128x5000)\n",
    "    frame1_descriptors = np.transpose(frame1_descriptors) # transpose -> current shape: 5000x128 - > 5000 points/queries each with 128 features/columns\n",
    "    #fit data of features from frame 1 to NearestNeighbour. When we ask for matches from this method, it should give us the 2 closest points to the point given\n",
    "    nbrs = NearestNeighbors(n_neighbors=2, algorithm='auto').fit(frame1_descriptors) \n",
    "\n",
    "    #predict matches for the other frame:\n",
    "    \n",
    "    frame_drescriptors = np.transpose(sift_points[s+1][2:,:]) #the same as done some lines above but for frame s+1\n",
    "    # Find the 2 nearest neighbors\n",
    "    distances, indices = nbrs.kneighbors(frame_drescriptors) \n",
    "    # indices is a 5000x2 shape matrix -> for each of the 5000 given feature points of frame_drescriptors it gives the 2 closest features from video frame 1\n",
    "    # distances is a measure of distance between the feature points of frame_drescriptors and each of the two givenneighbours from the indices matrix - it has the same size as indices\n",
    "    \n",
    "    features_matches=np.empty([4,0])\n",
    "    features_not_mateched=[]\n",
    "    for i in range(len(distances)): \n",
    "        if distances[i,0]< Threshold*distances[i,1] and distances[i,0]< 700:\n",
    "            #match is good for first neighbour found\n",
    "            features_matches= np.hstack((  features_matches   , np.array([[int(i)],[int(indices[i,0])], [distances[i,0]],[distances[i,1]]])  ))\n",
    "        else:                                                       # indice do frame s+1, indice do frame s, distâncias\n",
    "            #point is not good\n",
    "            features_not_mateched.append(i) #features from this frame that were not matched\n",
    "    \n",
    "    features_matches = features_matches[:, features_matches[1, :].argsort()] # this sorts the check_for_duplicates matrix in accordance to the values of it's second line\n",
    "    features_matches_deletedColumns= features_matches.copy()\n",
    "\n",
    "    for i in reversed (range (1, features_matches.shape[1])): #loop that starts in the last feature - because it deletes elements with their indexes from list check_for_duplicates_deletedColumns\n",
    "        # this has to be done starting from the end to not change the index of columns\n",
    "\n",
    "        # duplicates are adjacent because of sort\n",
    "        if features_matches[1,i-1] == features_matches[1,i]:\n",
    "            # if the value of the indice i and i-1 are equal, then there is one feature matched to 2 features of the new frame - we need to delete one of the matches\n",
    "            if features_matches[2,i-1] <= features_matches[2,i]: #check distance of i and i-1. And remove the one with the most distance\n",
    "                features_matches_deletedColumns= np.delete(features_matches_deletedColumns, i-1, 1) #remove duplicate feature matching (deletes one column - np dimension 1)\n",
    "                features_not_mateched.append(features_matches[0,i-1]) #append number of feature that was deleted to features not matched\n",
    "            else:\n",
    "                features_matches_deletedColumns= np.delete(features_matches_deletedColumns, i, 1) \n",
    "                features_not_mateched.append(features_matches[0,i]) \n",
    "    \n",
    "    matched_inThis_frame = features_matches_deletedColumns[:, features_matches_deletedColumns[0, :].argsort()] #to be in order in acoordance to index of frame s\n",
    "\n",
    "    matches.append( (matched_inThis_frame[0:2,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.000e+00, 2.000e+00, 3.000e+00, ..., 4.986e+03, 4.996e+03,\n",
       "        4.999e+03],\n",
       "       [6.650e+02, 4.133e+03, 1.332e+03, ..., 4.721e+03, 2.964e+03,\n",
       "        4.553e+03]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code **computes the Homography** between the frames of the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize(points):\n",
    "    mean = np.mean(points, axis=0)\n",
    "    std_dev = np.std(points)\n",
    "    T = np.array([[std_dev, 0, mean[0]], [0, std_dev, mean[1]], [0, 0, 1]])\n",
    "    T_inv = np.linalg.inv(T)\n",
    "    normalized_points = np.dot(T_inv, np.append(points, np.ones((points.shape[0], 1)), axis=1).T).T\n",
    "    return normalized_points, T\n",
    "\n",
    "def construct_matrix_A(points1, points2):\n",
    "    A = []\n",
    "    for i in range(points1.shape[0]):\n",
    "        x1, y1 = points1[i, 0], points1[i, 1]\n",
    "        x2, y2 = points2[i, 0], points2[i, 1]\n",
    "        A.append([-x1, -y1, -1, 0, 0, 0, x2*x1, x2*y1, x2])\n",
    "        A.append([0, 0, 0, -x1, -y1, -1, y2*x1, y2*y1, y2])\n",
    "    return np.array(A)\n",
    "\n",
    "def compute_homography(points1, points2):\n",
    "    points1_norm, T1 = normalize(points1)\n",
    "    points2_norm, T2 = normalize(points2)\n",
    "    A = construct_matrix_A(points1_norm, points2_norm)\n",
    "    _, _, V = np.linalg.svd(A)\n",
    "    H = V[-1].reshape(3, 3)\n",
    "    H = np.dot(np.linalg.inv(T2), np.dot(H, T1))\n",
    "    return H / H[2, 2]\n",
    "\n",
    "def normalize_points(points):\n",
    "    # Normalize points to have zero mean and unit variance\n",
    "    mean = np.mean(points, axis=0)\n",
    "    std = np.std(points, axis=0)\n",
    "    normalized_points = (points - mean) / std\n",
    "    return normalized_points, mean, std\n",
    "\n",
    "\n",
    "def denormalize_homography(H, mean_src, std_src, mean_dst, std_dst):\n",
    "    # Denormalize the homography matrix based on mean and standard deviation\n",
    "   # T_src = np.array([[1 / std_src[0], 0, -mean_src[0] / std_src[0]],\n",
    "            #          [0, 1 / std_src[1], -mean_src[1] / std_src[1]],\n",
    "            #          [0, 0, 1]])\n",
    "\n",
    "   # T_dst = np.array([[1 / std_dst[0], 0, -mean_dst[0] / std_dst[0]],\n",
    "   #                   [0, 1 / std_dst[1], -mean_dst[1] / std_dst[1]],\n",
    "   #                   [0, 0, 1]])\n",
    "    T_src = np.array([[std_src[0], 0, mean_src[0]], [0, std_src[1], mean_src[1]], [0, 0, 1]])\n",
    "    T_dst = np.array([[std_dst[0], 0, mean_dst[0]], [0, std_src[1], mean_dst[1]], [0, 0, 1]])\n",
    "\n",
    "    Homography = np.dot(np.linalg.inv(T_dst), np.dot(H, T_src))\n",
    "    \n",
    "    return  Homography\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "kp1 = kp_list[0]\n",
    "kp2 = kp_list[19]\n",
    "src_pts = np.float32([ kp1[q[0].queryIdx].pt for q in good ]).reshape(-1,1,2)\n",
    "dst_pts = np.float32([ kp2[t[0].trainIdx].pt for t in good ]).reshape(-1,1,2)\n",
    "src = np.reshape(src_pts,(np.shape(src_pts)[0],2))\n",
    "dst = np.reshape(dst_pts,(np.shape(dst_pts)[0],2))\n",
    "\n",
    "#src_pts_normalized, mean_src, std_src = normalize_points(src)\n",
    "#dst_pts_normalized, mean_dst, std_dst = normalize_points(dst)\n",
    "#src = preprocessing.normalize(src)   #Normalization\n",
    "#dst = preprocessing.normalize(dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Comp_H(src,dst):\n",
    "        A = []\n",
    "        for p, q in zip(src, dst):\n",
    "            x1 = p[0]\n",
    "            y1 = p[1]\n",
    "            x2 = q[0]\n",
    "            y2 = q[1]\n",
    "            A.append([-x1, -y1, -1, 0, 0, 0, x2*x1, x2*y1, x2])\n",
    "            A.append([0, 0, 0, -x1, -y1, -1, y2*x1, y2*y1, y2])\n",
    "\n",
    "        _, _, Vt = np.linalg.svd(A, full_matrices=True)\n",
    "        x = Vt[-1]\n",
    "        homography = x.reshape(3, -1) #/ x[-1]\n",
    "        return homography\n",
    "\n",
    "def RANSAC(Comp_H,src,dst,iter,threshold):\n",
    "      best_homography = None\n",
    "      inliers = [0]\n",
    "      for t in range(iter):\n",
    "            sample_indices = np.random.choice(int(len(src)), size=4, replace=False)\n",
    "            # Compute the Homography\n",
    "            H = Comp_H(src[sample_indices],dst[sample_indices])\n",
    "           # H = denormalize_homography(homography)\n",
    "            inl = 0\n",
    "            for p, q in zip(src, dst):\n",
    "                x1 = p[0]\n",
    "                y1 = p[1]\n",
    "                x2 = q[0]\n",
    "                y2 = q[1]\n",
    "            # Transform the point using the estimated homography\n",
    "                transformed_point = np.dot(H, np.array([x1, y1, 1]))\n",
    "\n",
    "            # Normalize the transformed point\n",
    "                transformed_point /= transformed_point[2]\n",
    "\n",
    "            # Calculate the Euclidean distance between the transformed point and the actual point\n",
    "                distance = np.linalg.norm(np.array([x2, y2, 1]) - transformed_point)\n",
    "                if distance < threshold:\n",
    "                   inl += 1\n",
    "            if inl > inliers[0]:\n",
    "                 best_homography = H\n",
    "                 inliers[0] = inl\n",
    "      return best_homography, inliers[0] \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "condition: 30609.206718393925 inliers:  44\n"
     ]
    }
   ],
   "source": [
    "H, inliers = RANSAC(Comp_H,src,dst,143,0.5)\n",
    "print('condition:',np.linalg.cond(H), 'inliers: ', inliers )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26395.908156677036"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#src_pts = np.float32([ kp1[all_matches[0][i][0]].pt for i in range(len(all_matches[0])) ]).reshape(-1,1,2)\n",
    "#dst_pts = np.float32([ kp2[all_matches[0][i][1]].pt for i in range(len(all_matches[0])) ]).reshape(-1,1,2)\n",
    "#M2, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "#np.linalg.cond(M2)\n",
    "\n",
    "#kp1 = kp_list[0]\n",
    "#kp2 = kp_list[1]\n",
    "#src_pts = np.float32([ kp1[q.queryIdx].pt for q in match[0] ]).reshape(-1,1,2)\n",
    "#dst_pts = np.float32([ kp2[t.trainIdx].pt for t in match[1] ]).reshape(-1,1,2)\n",
    "M, _ = cv2.findHomography(src, dst, cv2.RANSAC)\n",
    "np.linalg.cond(M)\n",
    "\n",
    "\n",
    "#picture2 = np.reshape(np.array([552,59,1]),(3,1))\n",
    "#picture1 = np.reshape(np.array([549,56,1]),(3,1))\n",
    "#pic_h = np.matmul(H,picture1)\n",
    "#print('the error is: \\n', pic_h/pic_h[2]-picture1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "homography = H\n",
    "\n",
    "def apply_homography(image, H):\n",
    "    \"\"\" Apply homography to the image \"\"\"\n",
    "    warped_img = cv2.warpPerspective(image, H, (image.shape[1], image.shape[0]))\n",
    "    return warped_img\n",
    "\n",
    "img_src = img1\n",
    "img_dest = img2\n",
    "\n",
    "warped_src = apply_homography(img_src, homography)\n",
    "#print('condition:',np.linalg.cond(H),'inliers: ', inliers)\n",
    "cv2.imshow('Warped Source Image', warped_src)\n",
    "cv2.imshow('Source Image', img_src)\n",
    "cv2.imshow('Destination Image', img_dest)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Zone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "861174.1382601217"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getPerspectiveTransform(src, dst):\n",
    "    if len(src) == len(dst):\n",
    "        # Make homogeneous coordiates if necessary\n",
    "        if src.shape[1] == 2:\n",
    "            src = np.hstack((src, np.ones((len(src), 1), dtype=src.dtype)))\n",
    "        if dst.shape[1] == 2:\n",
    "            dst = np.hstack((dst, np.ones((len(dst), 1), dtype=dst.dtype)))\n",
    "\n",
    "        # Solve 'Ax = 0'\n",
    "        A = []\n",
    "        for p, q in zip(src, dst):\n",
    "            A.append([0, 0, 0, q[2]*p[0], q[2]*p[1], q[2]*p[2], -q[1]*p[0], -q[1]*p[1], -q[1]*p[2]])\n",
    "            A.append([q[2]*p[0], q[2]*p[1], q[2]*p[2], 0, 0, 0, -q[0]*p[0], -q[0]*p[1], -q[0]*p[2]])\n",
    "\n",
    "        eigenvalue,eigenvector=eig(np.matmul(np.transpose(A),A))\n",
    "        _, _, Vt = np.linalg.svd(A, full_matrices=True)\n",
    "        x = Vt[-1]\n",
    "         \n",
    "\n",
    "        # Reorganize `x` as a matrix\n",
    "        H = x.reshape(3, -1) / x[-1] # Normalize the last element as 1\n",
    "        return H\n",
    "    \n",
    "\n",
    "H_slides = getPerspectiveTransform(np.reshape(src_pts,(np.shape(src_pts)[0],2)), np.reshape(dst_pts,(np.shape(dst_pts)[0],2)))\n",
    "np.linalg.cond(H_slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 422, y = 56\n",
      "x = 422, y = 56\n",
      "x = 422, y = 56\n",
      "x = 423, y = 56\n"
     ]
    }
   ],
   "source": [
    "def select_point(event,x,y,flags,param):\n",
    "    global ix,iy\n",
    "    if event == cv2.EVENT_LBUTTONDBLCLK: # captures left button double-click\n",
    "        print('x = %d, y = %d'%(x, y))\n",
    "\n",
    "cv2.namedWindow('frame')\n",
    "cv2.setMouseCallback('frame', select_point)\n",
    "cv2.imshow('frame',warped_src)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "capture = cv2.VideoCapture(os.path.abspath('trymefirst_lisbon.mp4'))\n",
    "framenr = 0 \n",
    "list_points = []\n",
    "while True:\n",
    "    success, frame = capture.read()\n",
    "    if success:\n",
    "        print('Current Frame!')\n",
    "        cv2.namedWindow('frame')\n",
    "        cv2.setMouseCallback('frame', select_point)\n",
    "        cv2.imshow('frame',frame)\n",
    "        if cv2.waitKey(0) & 0xFF == ord('q'):  ##press q if you want the video to stop \n",
    "             break\n",
    "        key = cv2.waitKey(0) & 0xFF == ord('k')\n",
    "        \n",
    "        print('New Frame!')\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
