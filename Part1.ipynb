{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Part 1** of the project is divided into three sections:\n",
    "\n",
    "1- Feature Extraction (Using SIFT)\n",
    "\n",
    "2- Outlier Removal (Using RANSAC)\n",
    "\n",
    "3- Computing the Homographies (Using DLT)\n",
    "\n",
    "\n",
    "**pip install opencv-python**\n",
    "\n",
    "**pip install opencv-contrib-python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from numpy.linalg import eig\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from src.extract_features import *\n",
    "from src.matching_features import *\n",
    "from src.homography import *\n",
    "from src.ransac import *\n",
    "from src.parsing import *\n",
    "from src.display_video import *\n",
    "from main import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image matches:  [] \n",
      "\n",
      "map matches:  [] \n",
      "\n"
     ]
    },
    {
     "ename": "LinAlgError",
     "evalue": "1-dimensional array given. Array must be at least two-dimensional",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLinAlgError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alexa\\OneDrive - Universidade de Lisboa\\4º Ano\\1º Semestre - MEEC\\PIV\\Project\\Git\\Image-Processing-and-Vision---Project\\Part1.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X26sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m match_img1 , match_map \u001b[39m=\u001b[39m parse_points(config_data) \u001b[39m#Parse the points from the configuration file\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X26sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m video_path \u001b[39m=\u001b[39m config_data[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mstrip() \u001b[39m#Get the video path\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X26sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m H_frame1_to_map \u001b[39m=\u001b[39mcompute_homography(match_img1, match_map)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X26sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(H_frame1_to_map)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X26sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCondition: \u001b[39m\u001b[39m\"\u001b[39m, np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mcond(H_frame1_to_map), \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\alexa\\OneDrive - Universidade de Lisboa\\4º Ano\\1º Semestre - MEEC\\PIV\\Project\\Git\\Image-Processing-and-Vision---Project\\src\\homography.py:18\u001b[0m, in \u001b[0;36mcompute_homography\u001b[1;34m(src, dst)\u001b[0m\n\u001b[0;32m     16\u001b[0m     A\u001b[39m.\u001b[39mappend([\u001b[39m-\u001b[39mx1, \u001b[39m-\u001b[39my1, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, x2\u001b[39m*\u001b[39mx1, x2\u001b[39m*\u001b[39my1, x2])\n\u001b[0;32m     17\u001b[0m     A\u001b[39m.\u001b[39mappend([\u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m-\u001b[39mx1, \u001b[39m-\u001b[39my1, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, y2\u001b[39m*\u001b[39mx1, y2\u001b[39m*\u001b[39my1, y2])\n\u001b[1;32m---> 18\u001b[0m _, _, Vt \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mlinalg\u001b[39m.\u001b[39;49msvd(A, full_matrices\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     19\u001b[0m x \u001b[39m=\u001b[39m Vt[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m     20\u001b[0m H \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mreshape(\u001b[39m3\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m/\u001b[39m x[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\alexa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\linalg\\linalg.py:1662\u001b[0m, in \u001b[0;36msvd\u001b[1;34m(a, full_matrices, compute_uv, hermitian)\u001b[0m\n\u001b[0;32m   1659\u001b[0m         s \u001b[39m=\u001b[39m \u001b[39mabs\u001b[39m(s)\n\u001b[0;32m   1660\u001b[0m         \u001b[39mreturn\u001b[39;00m sort(s)[\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m\u001b[39m.\u001b[39m, ::\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m-> 1662\u001b[0m _assert_stacked_2d(a)\n\u001b[0;32m   1663\u001b[0m t, result_t \u001b[39m=\u001b[39m _commonType(a)\n\u001b[0;32m   1665\u001b[0m extobj \u001b[39m=\u001b[39m get_linalg_error_extobj(_raise_linalgerror_svd_nonconvergence)\n",
      "File \u001b[1;32mc:\\Users\\alexa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\linalg\\linalg.py:206\u001b[0m, in \u001b[0;36m_assert_stacked_2d\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    204\u001b[0m \u001b[39mfor\u001b[39;00m a \u001b[39min\u001b[39;00m arrays:\n\u001b[0;32m    205\u001b[0m     \u001b[39mif\u001b[39;00m a\u001b[39m.\u001b[39mndim \u001b[39m<\u001b[39m \u001b[39m2\u001b[39m:\n\u001b[1;32m--> 206\u001b[0m         \u001b[39mraise\u001b[39;00m LinAlgError(\u001b[39m'\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m-dimensional array given. Array must be \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    207\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39mat least two-dimensional\u001b[39m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m a\u001b[39m.\u001b[39mndim)\n",
      "\u001b[1;31mLinAlgError\u001b[0m: 1-dimensional array given. Array must be at least two-dimensional"
     ]
    }
   ],
   "source": [
    "config_data = parse_configuration_file('config\\conf_file.cfg') #Parse the configuration file\n",
    "match_img1 , match_map = parse_points(config_data) #Parse the points from the configuration file\n",
    "video_path = config_data[0].split(' ')[1].strip() #Get the video path\n",
    "H_frame1_to_map =compute_homography(match_img1, match_map)\n",
    "print(H_frame1_to_map)\n",
    "print(\"Condition: \", np.linalg.cond(H_frame1_to_map), '\\n')\n",
    "\n",
    "sift_points, kp_list, img1, img2 = extract_features(video_path)\n",
    "#homography_two_frames(img1, img2, sift_points, kp_list, 1) #option 1 - with openCV; option 2 - with numpy\n",
    "\n",
    "match2 = matching_features_SCIKITLEARN(sift_points)\n",
    "print(match2)\n",
    "\n",
    "H_sequential = create_sequential_homographies(match2, sift_points)\n",
    "print('H_sequential' , H_sequential)\n",
    "H_output = homographie_to_map(H_sequential, H_frame1_to_map)\n",
    "print('H_output', H_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_frames(video_path):\n",
    "    \"\"\"Displays the video and counts the number of frames\"\"\"\n",
    "    capture = cv2.VideoCapture(os.path.abspath(video_path))\n",
    "    total_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"Total frames of the video: \", total_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code **extracts SIFT features** from each frame of the input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames of the video:  1901\n",
      "(Nº features, Nº descriptors per feature):  (5000, 128)\n",
      "Nº of frames extracted:  20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "capture = cv2.VideoCapture(os.path.abspath('Video/trymefirst_lisbon.mp4'))\n",
    "kp_list = []\n",
    "sift_points = [] #nome a definir no config\n",
    "t = 0 \n",
    "sift = cv2.SIFT_create(5000) #number of sift points\n",
    "img1, img2 = None, None\n",
    "k = 0\n",
    "count_frames(os.path.abspath('Video/trymefirst_lisbon.mp4'))\n",
    "while k <= 1900:\n",
    "        capture.set(cv2.CAP_PROP_POS_FRAMES, k)\n",
    "        success, frame = capture.read() #read the video\n",
    "        if success:\n",
    "            if (k == 0):\n",
    "                img1 = frame\n",
    "            if (k == 1900):\n",
    "                img2 = frame\n",
    "            frame_points = []\n",
    "            gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY) #convert image to gray\n",
    "            key_points, descriptors = sift.detectAndCompute(gray,None) \n",
    "            kp_list.append(key_points)\n",
    "            frame_points = ([key_points[0].pt[0],key_points[0].pt[1]]+descriptors[0].tolist())\n",
    "            for i in range(1,len(key_points)):\n",
    "                 temp_column = ([key_points[i].pt[0],key_points[i].pt[1]]+descriptors[i].tolist())\n",
    "                 frame_points = np.column_stack((frame_points,temp_column))  \n",
    "        sift_points.append(frame_points) #append everything into a list \n",
    "        k += 100\n",
    "print(\"(Nº features, Nº descriptors per feature): \", descriptors.shape)\n",
    "print(\"Nº of frames extracted: \", len(sift_points))\n",
    "#print(des.shape)\n",
    "#print(len(sift_points))\n",
    "#The keypoint is a point of interest in the image, the descriptor is a vector that describes the image patch around the keypoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code **matches SIFT features** between the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Brute force method\n",
    "bf = cv2.BFMatcher(crossCheck=True) #crossCheck is set to true so that the match is symmetric\n",
    "all_matches = []\n",
    "match = []\n",
    "for s in range(len(sift_points)-1):\n",
    "    point_matches = []\n",
    "\n",
    "    des1 = (((sift_points[s])[2:,:])).astype('float32')  # descriptors of the first frame\n",
    "    des2 = (((sift_points[s+1])[2:,:])).astype('float32')  # descriptors of the second\n",
    "    des1 = np.reshape(des1,(np.shape(des1)[1],128))\n",
    "    des2 = np.reshape(des2,(np.shape(des2)[1],128))\n",
    "\n",
    "    if np.shape(des1)[0] > np.shape(des2)[0]:\n",
    "             des1 = des1[:-abs(np.shape(des1)[0]-np.shape(des2)[0]),:]  # we are removing the last points so that we have an equal amount of SIFT features between two frames\n",
    "    if np.shape(des1)[0] < np.shape(des2)[0]:\n",
    "             des2 = des2[:-abs(np.shape(des1)[0]-np.shape(des2)[0]),:]\n",
    "    matches = bf.match(des1,des2)  # an error occurs if two frames have different amounts of SIFT features\n",
    "\n",
    "    for i in range(len(matches)):\n",
    "        match.append(matches)\n",
    "        point_matches.append([matches[i].queryIdx,matches[i].trainIdx])\n",
    "\n",
    "    all_matches.append(point_matches)\n",
    "\n",
    "    \n",
    "#Feature detection: opencv\n",
    "#Matching : sklearn , numpy\n",
    "#RANSAC: numpy\n",
    "#Create Homography: numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Feature matching using nearest neighbours, for pairs of consecutive frames\"\"\"\n",
    "    \n",
    "matches=[]\n",
    "Threshold=0.75\n",
    "\n",
    "for s in range(len(sift_points)-1):\n",
    "    frame1_descriptors = sift_points[s][2:,:] #descriptor values of every feature point for video frame s (current shape: 128x5000)\n",
    "    frame1_descriptors = np.transpose(frame1_descriptors) # transpose -> current shape: 5000x128 - > 5000 points/queries each with 128 features/columns\n",
    "    #fit data of features from frame 1 to NearestNeighbour. When we ask for matches from this method, it should give us the 2 closest points to the point given\n",
    "    nbrs = NearestNeighbors(n_neighbors=2, algorithm='auto').fit(frame1_descriptors) \n",
    "\n",
    "    #predict matches for the other frame:\n",
    "    \n",
    "    frame_drescriptors = np.transpose(sift_points[s+1][2:,:]) #the same as done some lines above but for frame s+1\n",
    "    # Find the 2 nearest neighbors\n",
    "    distances, indices = nbrs.kneighbors(frame_drescriptors) \n",
    "    # indices is a 5000x2 shape matrix -> for each of the 5000 given feature points of frame_drescriptors it gives the 2 closest features from video frame 1\n",
    "    # distances is a measure of distance between the feature points of frame_drescriptors and each of the two givenneighbours from the indices matrix - it has the same size as indices\n",
    "    \n",
    "    features_matches=np.empty([4,0])\n",
    "    features_not_mateched=[]\n",
    "    for i in range(len(distances)): \n",
    "        if distances[i,0]< Threshold*distances[i,1] and distances[i,0]< 700:\n",
    "            #match is good for first neighbour found\n",
    "            features_matches= np.hstack((  features_matches   , np.array([[int(i)],[int(indices[i,0])], [distances[i,0]],[distances[i,1]]])  ))\n",
    "        else:                                                       # indice do frame s+1, indice do frame s, distâncias\n",
    "            #point is not good\n",
    "            features_not_mateched.append(i) #features from this frame that were not matched\n",
    "    \n",
    "    features_matches = features_matches[:, features_matches[1, :].argsort()] # this sorts the check_for_duplicates matrix in accordance to the values of it's second line\n",
    "    features_matches_deletedColumns= features_matches.copy()\n",
    "\n",
    "    for i in reversed (range (1, features_matches.shape[1])): #loop that starts in the last feature - because it deletes elements with their indexes from list check_for_duplicates_deletedColumns\n",
    "        # this has to be done starting from the end to not change the index of columns\n",
    "\n",
    "        # duplicates are adjacent because of sort\n",
    "        if features_matches[1,i-1] == features_matches[1,i]:\n",
    "            # if the value of the indice i and i-1 are equal, then there is one feature matched to 2 features of the new frame - we need to delete one of the matches\n",
    "            if features_matches[2,i-1] <= features_matches[2,i]: #check distance of i and i-1. And remove the one with the most distance\n",
    "                features_matches_deletedColumns= np.delete(features_matches_deletedColumns, i-1, 1) #remove duplicate feature matching (deletes one column - np dimension 1)\n",
    "                features_not_mateched.append(features_matches[0,i-1]) #append number of feature that was deleted to features not matched\n",
    "            else:\n",
    "                features_matches_deletedColumns= np.delete(features_matches_deletedColumns, i, 1) \n",
    "                features_not_mateched.append(features_matches[0,i]) \n",
    "    \n",
    "    matched_inThis_frame = features_matches_deletedColumns[:, features_matches_deletedColumns[0, :].argsort()] #to be in order in acoordance to index of frame s\n",
    "\n",
    "    matches.append( (matched_inThis_frame[0:2,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.000e+00, 2.000e+00, 3.000e+00, ..., 4.986e+03, 4.996e+03,\n",
       "        4.999e+03],\n",
       "       [6.650e+02, 4.133e+03, 1.332e+03, ..., 4.721e+03, 2.964e+03,\n",
       "        4.553e+03]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code **computes the Homography** between the frames of the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize(points):\n",
    "    mean = np.mean(points, axis=0)\n",
    "    std_dev = np.std(points)\n",
    "    T = np.array([[std_dev, 0, mean[0]], [0, std_dev, mean[1]], [0, 0, 1]])\n",
    "    T_inv = np.linalg.inv(T)\n",
    "    normalized_points = np.dot(T_inv, np.append(points, np.ones((points.shape[0], 1)), axis=1).T).T\n",
    "    return normalized_points, T\n",
    "\n",
    "def construct_matrix_A(points1, points2):\n",
    "    A = []\n",
    "    for i in range(points1.shape[0]):\n",
    "        x1, y1 = points1[i, 0], points1[i, 1]\n",
    "        x2, y2 = points2[i, 0], points2[i, 1]\n",
    "        A.append([-x1, -y1, -1, 0, 0, 0, x2*x1, x2*y1, x2])\n",
    "        A.append([0, 0, 0, -x1, -y1, -1, y2*x1, y2*y1, y2])\n",
    "    return np.array(A)\n",
    "\n",
    "def compute_homography(points1, points2):\n",
    "    points1_norm, T1 = normalize(points1)\n",
    "    points2_norm, T2 = normalize(points2)\n",
    "    A = construct_matrix_A(points1_norm, points2_norm)\n",
    "    _, _, V = np.linalg.svd(A)\n",
    "    H = V[-1].reshape(3, 3)\n",
    "    H = np.dot(np.linalg.inv(T2), np.dot(H, T1))\n",
    "    return H / H[2, 2]\n",
    "\n",
    "def normalize_points(points):\n",
    "    # Normalize points to have zero mean and unit variance\n",
    "    mean = np.mean(points, axis=0)\n",
    "    std = np.std(points, axis=0)\n",
    "    normalized_points = (points - mean) / std\n",
    "    return normalized_points, mean, std\n",
    "\n",
    "\n",
    "def denormalize_homography(H, mean_src, std_src, mean_dst, std_dst):\n",
    "    # Denormalize the homography matrix based on mean and standard deviation\n",
    "   # T_src = np.array([[1 / std_src[0], 0, -mean_src[0] / std_src[0]],\n",
    "            #          [0, 1 / std_src[1], -mean_src[1] / std_src[1]],\n",
    "            #          [0, 0, 1]])\n",
    "\n",
    "   # T_dst = np.array([[1 / std_dst[0], 0, -mean_dst[0] / std_dst[0]],\n",
    "   #                   [0, 1 / std_dst[1], -mean_dst[1] / std_dst[1]],\n",
    "   #                   [0, 0, 1]])\n",
    "    T_src = np.array([[std_src[0], 0, mean_src[0]], [0, std_src[1], mean_src[1]], [0, 0, 1]])\n",
    "    T_dst = np.array([[std_dst[0], 0, mean_dst[0]], [0, std_src[1], mean_dst[1]], [0, 0, 1]])\n",
    "\n",
    "    Homography = np.dot(np.linalg.inv(T_dst), np.dot(H, T_src))\n",
    "    \n",
    "    return  Homography\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "kp1 = kp_list[0]\n",
    "kp2 = kp_list[19]\n",
    "src_pts = np.float32([ kp1[q[0].queryIdx].pt for q in good ]).reshape(-1,1,2)\n",
    "dst_pts = np.float32([ kp2[t[0].trainIdx].pt for t in good ]).reshape(-1,1,2)\n",
    "src = np.reshape(src_pts,(np.shape(src_pts)[0],2))\n",
    "dst = np.reshape(dst_pts,(np.shape(dst_pts)[0],2))\n",
    "\n",
    "#src_pts_normalized, mean_src, std_src = normalize_points(src)\n",
    "#dst_pts_normalized, mean_dst, std_dst = normalize_points(dst)\n",
    "#src = preprocessing.normalize(src)   #Normalization\n",
    "#dst = preprocessing.normalize(dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Comp_H(src,dst):\n",
    "        A = []\n",
    "        for p, q in zip(src, dst):\n",
    "            x1 = p[0]\n",
    "            y1 = p[1]\n",
    "            x2 = q[0]\n",
    "            y2 = q[1]\n",
    "            A.append([-x1, -y1, -1, 0, 0, 0, x2*x1, x2*y1, x2])\n",
    "            A.append([0, 0, 0, -x1, -y1, -1, y2*x1, y2*y1, y2])\n",
    "\n",
    "        _, _, Vt = np.linalg.svd(A, full_matrices=True)\n",
    "        x = Vt[-1]\n",
    "        homography = x.reshape(3, -1) #/ x[-1]\n",
    "        return homography\n",
    "\n",
    "def RANSAC(Comp_H,src,dst,iter,threshold):\n",
    "      best_homography = None\n",
    "      inliers = [0]\n",
    "      for t in range(iter):\n",
    "            sample_indices = np.random.choice(int(len(src)), size=4, replace=False)\n",
    "            # Compute the Homography\n",
    "            H = Comp_H(src[sample_indices],dst[sample_indices])\n",
    "           # H = denormalize_homography(homography)\n",
    "            inl = 0\n",
    "            for p, q in zip(src, dst):\n",
    "                x1 = p[0]\n",
    "                y1 = p[1]\n",
    "                x2 = q[0]\n",
    "                y2 = q[1]\n",
    "            # Transform the point using the estimated homography\n",
    "                transformed_point = np.dot(H, np.array([x1, y1, 1]))\n",
    "\n",
    "            # Normalize the transformed point\n",
    "                transformed_point /= transformed_point[2]\n",
    "\n",
    "            # Calculate the Euclidean distance between the transformed point and the actual point\n",
    "                distance = np.linalg.norm(np.array([x2, y2, 1]) - transformed_point)\n",
    "                if distance < threshold:\n",
    "                   inl += 1\n",
    "            if inl > inliers[0]:\n",
    "                 best_homography = H\n",
    "                 inliers[0] = inl\n",
    "      return best_homography, inliers[0] \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "condition: 30609.206718393925 inliers:  44\n"
     ]
    }
   ],
   "source": [
    "H, inliers = RANSAC(Comp_H,src,dst,143,0.5)\n",
    "print('condition:',np.linalg.cond(H), 'inliers: ', inliers )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26395.908156677036"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#src_pts = np.float32([ kp1[all_matches[0][i][0]].pt for i in range(len(all_matches[0])) ]).reshape(-1,1,2)\n",
    "#dst_pts = np.float32([ kp2[all_matches[0][i][1]].pt for i in range(len(all_matches[0])) ]).reshape(-1,1,2)\n",
    "#M2, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "#np.linalg.cond(M2)\n",
    "\n",
    "#kp1 = kp_list[0]\n",
    "#kp2 = kp_list[1]\n",
    "#src_pts = np.float32([ kp1[q.queryIdx].pt for q in match[0] ]).reshape(-1,1,2)\n",
    "#dst_pts = np.float32([ kp2[t.trainIdx].pt for t in match[1] ]).reshape(-1,1,2)\n",
    "M, _ = cv2.findHomography(src, dst, cv2.RANSAC)\n",
    "np.linalg.cond(M)\n",
    "\n",
    "\n",
    "#picture2 = np.reshape(np.array([552,59,1]),(3,1))\n",
    "#picture1 = np.reshape(np.array([549,56,1]),(3,1))\n",
    "#pic_h = np.matmul(H,picture1)\n",
    "#print('the error is: \\n', pic_h/pic_h[2]-picture1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "homography = H\n",
    "\n",
    "def apply_homography(image, H):\n",
    "    \"\"\" Apply homography to the image \"\"\"\n",
    "    warped_img = cv2.warpPerspective(image, H, (image.shape[1], image.shape[0]))\n",
    "    return warped_img\n",
    "\n",
    "img_src = img1\n",
    "img_dest = img2\n",
    "\n",
    "warped_src = apply_homography(img_src, homography)\n",
    "#print('condition:',np.linalg.cond(H),'inliers: ', inliers)\n",
    "cv2.imshow('Warped Source Image', warped_src)\n",
    "cv2.imshow('Source Image', img_src)\n",
    "cv2.imshow('Destination Image', img_dest)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Zone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "861174.1382601217"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getPerspectiveTransform(src, dst):\n",
    "    if len(src) == len(dst):\n",
    "        # Make homogeneous coordiates if necessary\n",
    "        if src.shape[1] == 2:\n",
    "            src = np.hstack((src, np.ones((len(src), 1), dtype=src.dtype)))\n",
    "        if dst.shape[1] == 2:\n",
    "            dst = np.hstack((dst, np.ones((len(dst), 1), dtype=dst.dtype)))\n",
    "\n",
    "        # Solve 'Ax = 0'\n",
    "        A = []\n",
    "        for p, q in zip(src, dst):\n",
    "            A.append([0, 0, 0, q[2]*p[0], q[2]*p[1], q[2]*p[2], -q[1]*p[0], -q[1]*p[1], -q[1]*p[2]])\n",
    "            A.append([q[2]*p[0], q[2]*p[1], q[2]*p[2], 0, 0, 0, -q[0]*p[0], -q[0]*p[1], -q[0]*p[2]])\n",
    "\n",
    "        eigenvalue,eigenvector=eig(np.matmul(np.transpose(A),A))\n",
    "        _, _, Vt = np.linalg.svd(A, full_matrices=True)\n",
    "        x = Vt[-1]\n",
    "         \n",
    "\n",
    "        # Reorganize `x` as a matrix\n",
    "        H = x.reshape(3, -1) / x[-1] # Normalize the last element as 1\n",
    "        return H\n",
    "    \n",
    "\n",
    "H_slides = getPerspectiveTransform(np.reshape(src_pts,(np.shape(src_pts)[0],2)), np.reshape(dst_pts,(np.shape(dst_pts)[0],2)))\n",
    "np.linalg.cond(H_slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 422, y = 56\n",
      "x = 422, y = 56\n",
      "x = 422, y = 56\n",
      "x = 423, y = 56\n"
     ]
    }
   ],
   "source": [
    "def select_point(event,x,y,flags,param):\n",
    "    global ix,iy\n",
    "    if event == cv2.EVENT_LBUTTONDBLCLK: # captures left button double-click\n",
    "        print('x = %d, y = %d'%(x, y))\n",
    "\n",
    "cv2.namedWindow('frame')\n",
    "cv2.setMouseCallback('frame', select_point)\n",
    "cv2.imshow('frame',warped_src)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "capture = cv2.VideoCapture(os.path.abspath('trymefirst_lisbon.mp4'))\n",
    "framenr = 0 \n",
    "list_points = []\n",
    "while True:\n",
    "    success, frame = capture.read()\n",
    "    if success:\n",
    "        print('Current Frame!')\n",
    "        cv2.namedWindow('frame')\n",
    "        cv2.setMouseCallback('frame', select_point)\n",
    "        cv2.imshow('frame',frame)\n",
    "        if cv2.waitKey(0) & 0xFF == ord('q'):  ##press q if you want the video to stop \n",
    "             break\n",
    "        key = cv2.waitKey(0) & 0xFF == ord('k')\n",
    "        \n",
    "        print('New Frame!')\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
