{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **Part 1** of the project is divided into three sections:\n",
    "\n",
    "1- Feature Extraction (Using SIFT)\n",
    "\n",
    "2- Outlier Removal (Using RANSAC)\n",
    "\n",
    "3- Computing the Homographies (Using DLT)\n",
    "\n",
    "\n",
    "**pip install opencv-python**\n",
    "\n",
    "**pip install opencv-contrib-python**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "from numpy.linalg import eig\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from src.extract_features import *\n",
    "from src.matching_features import *\n",
    "from src.homography import *\n",
    "from src.ransac import *\n",
    "from src.parsing import *\n",
    "from src.display_video import *\n",
    "from main import*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image matches:  [('225', '131'), ('580', '120'), ('626', '305'), ('133', '303')] \n",
      "\n",
      "map matches:  [('225', '131'), ('580', '120'), ('626', '305'), ('133', '303')] \n",
      "\n",
      "[[ 1.00000000e+00 -1.97777988e-13  5.53181623e-11]\n",
      " [ 2.38054129e-14  1.00000000e+00  1.91816403e-11]\n",
      " [ 6.47482617e-17 -8.45291342e-16  1.00000000e+00]]\n",
      "Condition:  1.0000000000585505 \n",
      "\n",
      "video/trymefirst_lisbon.mp4\n",
      "Total frames of the video:  1901\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alexa\\OneDrive - Universidade de Lisboa\\4º Ano\\1º Semestre - MEEC\\PIV\\Project\\Git\\Image-Processing-and-Vision---Project\\Part1.ipynb Cell 4\u001b[0m line \u001b[0;36m8\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#W3sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mprint\u001b[39m(H_frame1_to_map)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#W3sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mCondition: \u001b[39m\u001b[39m\"\u001b[39m, np\u001b[39m.\u001b[39mlinalg\u001b[39m.\u001b[39mcond(H_frame1_to_map), \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#W3sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m sift_points, kp_list, img1, img2 \u001b[39m=\u001b[39m extract_features(video_path)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#W3sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#homography_two_frames(img1, img2, sift_points, kp_list, 1) #option 1 - with openCV; option 2 - with numpy\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#W3sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m match \u001b[39m=\u001b[39m matching_features_SCIKITLEARN(sift_points)\n",
      "File \u001b[1;32mc:\\Users\\alexa\\OneDrive - Universidade de Lisboa\\4º Ano\\1º Semestre - MEEC\\PIV\\Project\\Git\\Image-Processing-and-Vision---Project\\src\\extract_features.py:40\u001b[0m, in \u001b[0;36mextract_features\u001b[1;34m(video_path)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39mlen\u001b[39m(key_points)):\n\u001b[0;32m     39\u001b[0m          temp_column \u001b[39m=\u001b[39m ([key_points[i]\u001b[39m.\u001b[39mpt[\u001b[39m0\u001b[39m],key_points[i]\u001b[39m.\u001b[39mpt[\u001b[39m1\u001b[39m]]\u001b[39m+\u001b[39mdescriptors[i]\u001b[39m.\u001b[39mtolist())\n\u001b[1;32m---> 40\u001b[0m          frame_points \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mcolumn_stack((frame_points,temp_column))  \n\u001b[0;32m     41\u001b[0m sift_points\u001b[39m.\u001b[39mappend(frame_points) \u001b[39m#append everything into a list \u001b[39;00m\n\u001b[0;32m     42\u001b[0m k \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m100\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\alexa\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\numpy\\lib\\shape_base.py:652\u001b[0m, in \u001b[0;36mcolumn_stack\u001b[1;34m(tup)\u001b[0m\n\u001b[0;32m    650\u001b[0m         arr \u001b[39m=\u001b[39m array(arr, copy\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, subok\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, ndmin\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\u001b[39m.\u001b[39mT\n\u001b[0;32m    651\u001b[0m     arrays\u001b[39m.\u001b[39mappend(arr)\n\u001b[1;32m--> 652\u001b[0m \u001b[39mreturn\u001b[39;00m _nx\u001b[39m.\u001b[39;49mconcatenate(arrays, \u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "config_data = parse_configuration_file('config\\part_1.cfg') #Parse the configuration file\n",
    "match_img1 , match_map = parse_points(config_data) #Parse the points from the configuration file\n",
    "video_path = config_data[0].split(' ')[1].strip() #Get the video path\n",
    "H_frame1_to_map =compute_homography(match_img1, match_map)\n",
    "print(H_frame1_to_map)\n",
    "print(\"Condition: \", np.linalg.cond(H_frame1_to_map), '\\n')\n",
    "\n",
    "sift_points, kp_list, img1, img2 = extract_features(video_path)\n",
    "#homography_two_frames(img1, img2, sift_points, kp_list, 1) #option 1 - with openCV; option 2 - with numpy\n",
    "\n",
    "match = matching_features_SCIKITLEARN(sift_points)\n",
    "# print(match2)\n",
    "\n",
    "H_sequential = create_sequential_homographies(match, sift_points)\n",
    "print('H_sequential' , H_sequential)\n",
    "H_output = homography_to_map(H_sequential, H_frame1_to_map)\n",
    "print('H_output', H_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "H_output=np.empty([11,0])\n",
    "H_i = np.vstack((np.array([[0], [1]]) , H_frame1_to_map.reshape(9,1) )) #first part of the array is 0 and 1 - which means homography from frame 1 to map (frame 0)\n",
    "H_output = np.hstack([H_output, H_i])\n",
    "                        \n",
    "for i in range(1, len(H_sequential)):\n",
    "    T_to_map= np.matmul(  H_output[2:,i-1].reshape(3,3), H_sequential[2:,i-1].reshape(3,3)) \n",
    "    #for frame n, H_output[2:,i-1] should be the homography from frame n-1 to the map. \n",
    "    # H_sequential[2:,i-1] should be the homography from frame n to n-1\n",
    "    # So T_to_Map should be the homography from frame n to map\n",
    "\n",
    "    H_i = np.vstack(( np.array([[0],[H_sequential[1,i-1]]] ), T_to_map.reshape(9,1) ))\n",
    "\n",
    "    H_output = np.hstack([H_output, H_i])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_features_frames(video_path= 'video/trymefirst_lisbon.mp4'):\n",
    "    \"\"\"Extracts the features from the video and stores them in a list\"\"\"\n",
    "    print(video_path)\n",
    "    capture = cv2.VideoCapture(os.path.abspath(video_path))\n",
    "    kp_list = []\n",
    "    sift_points = [] #nome a definir no config\n",
    "    sift = cv2.SIFT_create(5000) #number of sift points\n",
    "    img1, img2 = None, None\n",
    "    k = 0\n",
    "    frames=[]\n",
    "    count_frames(video_path)\n",
    "    while k <= 1900:\n",
    "        capture.set(cv2.CAP_PROP_POS_FRAMES, k)\n",
    "        success, frame = capture.read() #read the video\n",
    "        if success:\n",
    "            if (k == 0):\n",
    "                img1 = frame\n",
    "            if (k == 1900):\n",
    "                img2 = frame\n",
    "            frame_points = []\n",
    "            frames.append(frame)\n",
    "            gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY) #convert image to gray\n",
    "            key_points, descriptors = sift.detectAndCompute(gray,None) \n",
    "            kp_list.append(key_points)\n",
    "            frame_points = ([key_points[0].pt[0],key_points[0].pt[1]]+descriptors[0].tolist())\n",
    "            for i in range(1,len(key_points)):\n",
    "                 temp_column = ([key_points[i].pt[0],key_points[i].pt[1]]+descriptors[i].tolist())\n",
    "                 frame_points = np.column_stack((frame_points,temp_column))  \n",
    "        sift_points.append(frame_points) #append everything into a list \n",
    "        k += 100\n",
    "    print(\"(Nº features, Nº descriptors per feature): \", descriptors.shape)\n",
    "    print(\"Nº of frames extracted: \", len(sift_points))\n",
    "    return frames\n",
    "    \n",
    "def show_pixel_value(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        # Get the BGR values at the clicked position\n",
    "        b, g, r = img[y, x]\n",
    "        print(f\"Pixel value at (x={x}, y={y}): B={b}, G={g}, R={r}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def display(frame1, frame2,homography_de2_para1 ):\n",
    "    \n",
    "\n",
    "    # Display frame 1\n",
    "    H= homography_de2_para1.reshape((3,3))\n",
    "    \n",
    "\n",
    "    height, width = frame2.shape[:2]\n",
    "    warped_frame2 = cv2.warpPerspective(frame2, H, (width, height))\n",
    "\n",
    "    # Apply homography to frame 2\n",
    "\n",
    "    # Display frame 2 with homography applied\n",
    "    while True:\n",
    "        cv2.namedWindow('Image')\n",
    "        cv2.setMouseCallback('Image', show_pixel_value)\n",
    "        cv2.imshow(\"Frame 1\", frame1)\n",
    "        cv2.namedWindow('Image2')\n",
    "        cv2.setMouseCallback('Image2', show_pixel_value)\n",
    "        cv2.imshow(\"Frame 2 with Homography\", warped_frame2)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "video/trymefirst_lisbon.mp4\n",
      "Total frames of the video:  1901\n",
      "(Nº features, Nº descriptors per feature):  (5000, 128)\n",
      "Nº of frames extracted:  20\n"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames= extract_features_frames()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alexa\\OneDrive - Universidade de Lisboa\\4º Ano\\1º Semestre - MEEC\\PIV\\Project\\Git\\Image-Processing-and-Vision---Project\\Part1.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow_pixel_value\u001b[39m(event, x, y, flags, param):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39mif\u001b[39;00m event \u001b[39m==\u001b[39m cv2\u001b[39m.\u001b[39mEVENT_LBUTTONDOWN:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m         \u001b[39m# Get the BGR values at the clicked position\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         b, g, r \u001b[39m=\u001b[39m event[y, x]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPixel value at (x=\u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m, y=\u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m}\u001b[39;00m\u001b[39m): B=\u001b[39m\u001b[39m{\u001b[39;00mb\u001b[39m}\u001b[39;00m\u001b[39m, G=\u001b[39m\u001b[39m{\u001b[39;00mg\u001b[39m}\u001b[39;00m\u001b[39m, R=\u001b[39m\u001b[39m{\u001b[39;00mr\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alexa\\OneDrive - Universidade de Lisboa\\4º Ano\\1º Semestre - MEEC\\PIV\\Project\\Git\\Image-Processing-and-Vision---Project\\Part1.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow_pixel_value\u001b[39m(event, x, y, flags, param):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39mif\u001b[39;00m event \u001b[39m==\u001b[39m cv2\u001b[39m.\u001b[39mEVENT_LBUTTONDOWN:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m         \u001b[39m# Get the BGR values at the clicked position\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         b, g, r \u001b[39m=\u001b[39m event[y, x]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPixel value at (x=\u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m, y=\u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m}\u001b[39;00m\u001b[39m): B=\u001b[39m\u001b[39m{\u001b[39;00mb\u001b[39m}\u001b[39;00m\u001b[39m, G=\u001b[39m\u001b[39m{\u001b[39;00mg\u001b[39m}\u001b[39;00m\u001b[39m, R=\u001b[39m\u001b[39m{\u001b[39;00mr\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alexa\\OneDrive - Universidade de Lisboa\\4º Ano\\1º Semestre - MEEC\\PIV\\Project\\Git\\Image-Processing-and-Vision---Project\\Part1.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow_pixel_value\u001b[39m(event, x, y, flags, param):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39mif\u001b[39;00m event \u001b[39m==\u001b[39m cv2\u001b[39m.\u001b[39mEVENT_LBUTTONDOWN:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m         \u001b[39m# Get the BGR values at the clicked position\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         b, g, r \u001b[39m=\u001b[39m event[y, x]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPixel value at (x=\u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m, y=\u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m}\u001b[39;00m\u001b[39m): B=\u001b[39m\u001b[39m{\u001b[39;00mb\u001b[39m}\u001b[39;00m\u001b[39m, G=\u001b[39m\u001b[39m{\u001b[39;00mg\u001b[39m}\u001b[39;00m\u001b[39m, R=\u001b[39m\u001b[39m{\u001b[39;00mr\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alexa\\OneDrive - Universidade de Lisboa\\4º Ano\\1º Semestre - MEEC\\PIV\\Project\\Git\\Image-Processing-and-Vision---Project\\Part1.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow_pixel_value\u001b[39m(event, x, y, flags, param):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39mif\u001b[39;00m event \u001b[39m==\u001b[39m cv2\u001b[39m.\u001b[39mEVENT_LBUTTONDOWN:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m         \u001b[39m# Get the BGR values at the clicked position\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         b, g, r \u001b[39m=\u001b[39m event[y, x]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPixel value at (x=\u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m, y=\u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m}\u001b[39;00m\u001b[39m): B=\u001b[39m\u001b[39m{\u001b[39;00mb\u001b[39m}\u001b[39;00m\u001b[39m, G=\u001b[39m\u001b[39m{\u001b[39;00mg\u001b[39m}\u001b[39;00m\u001b[39m, R=\u001b[39m\u001b[39m{\u001b[39;00mr\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alexa\\OneDrive - Universidade de Lisboa\\4º Ano\\1º Semestre - MEEC\\PIV\\Project\\Git\\Image-Processing-and-Vision---Project\\Part1.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow_pixel_value\u001b[39m(event, x, y, flags, param):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39mif\u001b[39;00m event \u001b[39m==\u001b[39m cv2\u001b[39m.\u001b[39mEVENT_LBUTTONDOWN:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m         \u001b[39m# Get the BGR values at the clicked position\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         b, g, r \u001b[39m=\u001b[39m event[y, x]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPixel value at (x=\u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m, y=\u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m}\u001b[39;00m\u001b[39m): B=\u001b[39m\u001b[39m{\u001b[39;00mb\u001b[39m}\u001b[39;00m\u001b[39m, G=\u001b[39m\u001b[39m{\u001b[39;00mg\u001b[39m}\u001b[39;00m\u001b[39m, R=\u001b[39m\u001b[39m{\u001b[39;00mr\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alexa\\OneDrive - Universidade de Lisboa\\4º Ano\\1º Semestre - MEEC\\PIV\\Project\\Git\\Image-Processing-and-Vision---Project\\Part1.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow_pixel_value\u001b[39m(event, x, y, flags, param):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39mif\u001b[39;00m event \u001b[39m==\u001b[39m cv2\u001b[39m.\u001b[39mEVENT_LBUTTONDOWN:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m         \u001b[39m# Get the BGR values at the clicked position\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         b, g, r \u001b[39m=\u001b[39m event[y, x]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPixel value at (x=\u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m, y=\u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m}\u001b[39;00m\u001b[39m): B=\u001b[39m\u001b[39m{\u001b[39;00mb\u001b[39m}\u001b[39;00m\u001b[39m, G=\u001b[39m\u001b[39m{\u001b[39;00mg\u001b[39m}\u001b[39;00m\u001b[39m, R=\u001b[39m\u001b[39m{\u001b[39;00mr\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'int' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alexa\\OneDrive - Universidade de Lisboa\\4º Ano\\1º Semestre - MEEC\\PIV\\Project\\Git\\Image-Processing-and-Vision---Project\\Part1.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshow_pixel_value\u001b[39m(event, x, y, flags, param):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     \u001b[39mif\u001b[39;00m event \u001b[39m==\u001b[39m cv2\u001b[39m.\u001b[39mEVENT_LBUTTONDOWN:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m         \u001b[39m# Get the BGR values at the clicked position\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m         b, g, r \u001b[39m=\u001b[39m event[y, x]\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mPixel value at (x=\u001b[39m\u001b[39m{\u001b[39;00mx\u001b[39m}\u001b[39;00m\u001b[39m, y=\u001b[39m\u001b[39m{\u001b[39;00my\u001b[39m}\u001b[39;00m\u001b[39m): B=\u001b[39m\u001b[39m{\u001b[39;00mb\u001b[39m}\u001b[39;00m\u001b[39m, G=\u001b[39m\u001b[39m{\u001b[39;00mg\u001b[39m}\u001b[39;00m\u001b[39m, R=\u001b[39m\u001b[39m{\u001b[39;00mr\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'int' object is not subscriptable"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 11 is out of bounds for axis 1 with size 11",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alexa\\OneDrive - Universidade de Lisboa\\4º Ano\\1º Semestre - MEEC\\PIV\\Project\\Git\\Image-Processing-and-Vision---Project\\Part1.ipynb Cell 9\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m,\u001b[39mlen\u001b[39m(frames)\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     img2\u001b[39m=\u001b[39m frames[i]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     display(img1, img2, H_output[\u001b[39m2\u001b[39;49m:,i] )\n",
      "\u001b[1;31mIndexError\u001b[0m: index 11 is out of bounds for axis 1 with size 11"
     ]
    }
   ],
   "source": [
    "#to test homography\n",
    "img1=frames[0]\n",
    "for i in range(1,len(frames)-1):\n",
    "    img2= frames[i]\n",
    "    display(img1, img2, H_output[2:,i] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\alexa\\OneDrive - Universidade de Lisboa\\4º Ano\\1º Semestre - MEEC\\PIV\\Project\\Git\\Image-Processing-and-Vision---Project\\Part1.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X31sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmain\u001b[39;00m \u001b[39mimport\u001b[39;00m\u001b[39m*\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X31sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m H_output \u001b[39m=\u001b[39m homography_to_map(H_sequential, H_frame1_to_map)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/alexa/OneDrive%20-%20Universidade%20de%20Lisboa/4%C2%BA%20Ano/1%C2%BA%20Semestre%20-%20MEEC/PIV/Project/Git/Image-Processing-and-Vision---Project/Part1.ipynb#X31sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mH_output\u001b[39m\u001b[39m'\u001b[39m, H_output)\n",
      "File \u001b[1;32mc:\\Users\\alexa\\OneDrive - Universidade de Lisboa\\4º Ano\\1º Semestre - MEEC\\PIV\\Project\\Git\\Image-Processing-and-Vision---Project\\main.py:54\u001b[0m, in \u001b[0;36mhomography_to_map\u001b[1;34m(H_sequential, H_frame1_to_map)\u001b[0m\n\u001b[0;32m     51\u001b[0m H_i \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mvstack((np\u001b[39m.\u001b[39marray([[\u001b[39m0\u001b[39m], [\u001b[39m1\u001b[39m]]) , H_frame1_to_map\u001b[39m.\u001b[39mreshape(\u001b[39m9\u001b[39m,\u001b[39m1\u001b[39m) )) \u001b[39m#first part of the array is 0 and 1 - which means homography from frame 1 to map (frame 0)\u001b[39;00m\n\u001b[0;32m     52\u001b[0m H_output \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mhstack([H_output, H_i])\n\u001b[1;32m---> 54\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m1\u001b[39m, H_sequential\u001b[39m.\u001b[39;49mshape(\u001b[39m1\u001b[39;49m)):\n\u001b[0;32m     55\u001b[0m     T_to_map\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmatmul(  H_output[\u001b[39m2\u001b[39m:,i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mreshape(\u001b[39m3\u001b[39m,\u001b[39m3\u001b[39m), H_sequential[\u001b[39m2\u001b[39m:,i\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mreshape(\u001b[39m3\u001b[39m,\u001b[39m3\u001b[39m)) \n\u001b[0;32m     56\u001b[0m     \u001b[39m#for frame n, H_output[2:,i-1] should be the homography from frame n-1 to the map. \u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[39m# H_sequential[2:,i-1] should be the homography from frame n to n-1\u001b[39;00m\n\u001b[0;32m     58\u001b[0m     \u001b[39m# So T_to_Map should be the homography from frame n to map\u001b[39;00m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'tuple' object is not callable"
     ]
    }
   ],
   "source": [
    "from main import*\n",
    "H_output = homography_to_map(H_sequential, H_frame1_to_map)\n",
    "print('H_output', H_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_frames(video_path):\n",
    "    \"\"\"Displays the video and counts the number of frames\"\"\"\n",
    "    capture = cv2.VideoCapture(os.path.abspath(video_path))\n",
    "    total_frames = int(capture.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "    print(\"Total frames of the video: \", total_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code **extracts SIFT features** from each frame of the input video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total frames of the video:  1901\n",
      "(Nº features, Nº descriptors per feature):  (5000, 128)\n",
      "Nº of frames extracted:  20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "capture = cv2.VideoCapture(os.path.abspath('Video/trymefirst_lisbon.mp4'))\n",
    "kp_list = []\n",
    "sift_points = [] #nome a definir no config\n",
    "t = 0 \n",
    "sift = cv2.SIFT_create(5000) #number of sift points\n",
    "img1, img2 = None, None\n",
    "k = 0\n",
    "count_frames(os.path.abspath('Video/trymefirst_lisbon.mp4'))\n",
    "while k <= 1900:\n",
    "        capture.set(cv2.CAP_PROP_POS_FRAMES, k)\n",
    "        success, frame = capture.read() #read the video\n",
    "        if success:\n",
    "            if (k == 0):\n",
    "                img1 = frame\n",
    "            if (k == 1900):\n",
    "                img2 = frame\n",
    "            frame_points = []\n",
    "            gray = cv2.cvtColor(frame,cv2.COLOR_BGR2GRAY) #convert image to gray\n",
    "            key_points, descriptors = sift.detectAndCompute(gray,None) \n",
    "            kp_list.append(key_points)\n",
    "            frame_points = ([key_points[0].pt[0],key_points[0].pt[1]]+descriptors[0].tolist())\n",
    "            for i in range(1,len(key_points)):\n",
    "                 temp_column = ([key_points[i].pt[0],key_points[i].pt[1]]+descriptors[i].tolist())\n",
    "                 frame_points = np.column_stack((frame_points,temp_column))  \n",
    "        sift_points.append(frame_points) #append everything into a list \n",
    "        k += 100\n",
    "print(\"(Nº features, Nº descriptors per feature): \", descriptors.shape)\n",
    "print(\"Nº of frames extracted: \", len(sift_points))\n",
    "#print(des.shape)\n",
    "#print(len(sift_points))\n",
    "#The keypoint is a point of interest in the image, the descriptor is a vector that describes the image patch around the keypoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code **matches SIFT features** between the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Brute force method\n",
    "bf = cv2.BFMatcher(crossCheck=True) #crossCheck is set to true so that the match is symmetric\n",
    "all_matches = []\n",
    "match = []\n",
    "for s in range(len(sift_points)-1):\n",
    "    point_matches = []\n",
    "\n",
    "    des1 = (((sift_points[s])[2:,:])).astype('float32')  # descriptors of the first frame\n",
    "    des2 = (((sift_points[s+1])[2:,:])).astype('float32')  # descriptors of the second\n",
    "    des1 = np.reshape(des1,(np.shape(des1)[1],128))\n",
    "    des2 = np.reshape(des2,(np.shape(des2)[1],128))\n",
    "\n",
    "    if np.shape(des1)[0] > np.shape(des2)[0]:\n",
    "             des1 = des1[:-abs(np.shape(des1)[0]-np.shape(des2)[0]),:]  # we are removing the last points so that we have an equal amount of SIFT features between two frames\n",
    "    if np.shape(des1)[0] < np.shape(des2)[0]:\n",
    "             des2 = des2[:-abs(np.shape(des1)[0]-np.shape(des2)[0]),:]\n",
    "    matches = bf.match(des1,des2)  # an error occurs if two frames have different amounts of SIFT features\n",
    "\n",
    "    for i in range(len(matches)):\n",
    "        match.append(matches)\n",
    "        point_matches.append([matches[i].queryIdx,matches[i].trainIdx])\n",
    "\n",
    "    all_matches.append(point_matches)\n",
    "\n",
    "    \n",
    "#Feature detection: opencv\n",
    "#Matching : sklearn , numpy\n",
    "#RANSAC: numpy\n",
    "#Create Homography: numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Feature matching using nearest neighbours, for pairs of consecutive frames\"\"\"\n",
    "    \n",
    "matches=[]\n",
    "Threshold=0.75\n",
    "\n",
    "for s in range(len(sift_points)-1):\n",
    "    frame1_descriptors = sift_points[s][2:,:] #descriptor values of every feature point for video frame s (current shape: 128x5000)\n",
    "    frame1_descriptors = np.transpose(frame1_descriptors) # transpose -> current shape: 5000x128 - > 5000 points/queries each with 128 features/columns\n",
    "    #fit data of features from frame 1 to NearestNeighbour. When we ask for matches from this method, it should give us the 2 closest points to the point given\n",
    "    nbrs = NearestNeighbors(n_neighbors=2, algorithm='auto').fit(frame1_descriptors) \n",
    "\n",
    "    #predict matches for the other frame:\n",
    "    \n",
    "    frame_drescriptors = np.transpose(sift_points[s+1][2:,:]) #the same as done some lines above but for frame s+1\n",
    "    # Find the 2 nearest neighbors\n",
    "    distances, indices = nbrs.kneighbors(frame_drescriptors) \n",
    "    # indices is a 5000x2 shape matrix -> for each of the 5000 given feature points of frame_drescriptors it gives the 2 closest features from video frame 1\n",
    "    # distances is a measure of distance between the feature points of frame_drescriptors and each of the two givenneighbours from the indices matrix - it has the same size as indices\n",
    "    \n",
    "    features_matches=np.empty([4,0])\n",
    "    features_not_mateched=[]\n",
    "    for i in range(len(distances)): \n",
    "        if distances[i,0]< Threshold*distances[i,1] and distances[i,0]< 700:\n",
    "            #match is good for first neighbour found\n",
    "            features_matches= np.hstack((  features_matches   , np.array([[int(i)],[int(indices[i,0])], [distances[i,0]],[distances[i,1]]])  ))\n",
    "        else:                                                       # indice do frame s+1, indice do frame s, distâncias\n",
    "            #point is not good\n",
    "            features_not_mateched.append(i) #features from this frame that were not matched\n",
    "    \n",
    "    features_matches = features_matches[:, features_matches[1, :].argsort()] # this sorts the check_for_duplicates matrix in accordance to the values of it's second line\n",
    "    features_matches_deletedColumns= features_matches.copy()\n",
    "\n",
    "    for i in reversed (range (1, features_matches.shape[1])): #loop that starts in the last feature - because it deletes elements with their indexes from list check_for_duplicates_deletedColumns\n",
    "        # this has to be done starting from the end to not change the index of columns\n",
    "\n",
    "        # duplicates are adjacent because of sort\n",
    "        if features_matches[1,i-1] == features_matches[1,i]:\n",
    "            # if the value of the indice i and i-1 are equal, then there is one feature matched to 2 features of the new frame - we need to delete one of the matches\n",
    "            if features_matches[2,i-1] <= features_matches[2,i]: #check distance of i and i-1. And remove the one with the most distance\n",
    "                features_matches_deletedColumns= np.delete(features_matches_deletedColumns, i-1, 1) #remove duplicate feature matching (deletes one column - np dimension 1)\n",
    "                features_not_mateched.append(features_matches[0,i-1]) #append number of feature that was deleted to features not matched\n",
    "            else:\n",
    "                features_matches_deletedColumns= np.delete(features_matches_deletedColumns, i, 1) \n",
    "                features_not_mateched.append(features_matches[0,i]) \n",
    "    \n",
    "    matched_inThis_frame = features_matches_deletedColumns[:, features_matches_deletedColumns[0, :].argsort()] #to be in order in acoordance to index of frame s\n",
    "\n",
    "    matches.append( (matched_inThis_frame[0:2,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.000e+00, 2.000e+00, 3.000e+00, ..., 4.986e+03, 4.996e+03,\n",
       "        4.999e+03],\n",
       "       [6.650e+02, 4.133e+03, 1.332e+03, ..., 4.721e+03, 2.964e+03,\n",
       "        4.553e+03]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matches[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code **computes the Homography** between the frames of the video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def normalize(points):\n",
    "    mean = np.mean(points, axis=0)\n",
    "    std_dev = np.std(points)\n",
    "    T = np.array([[std_dev, 0, mean[0]], [0, std_dev, mean[1]], [0, 0, 1]])\n",
    "    T_inv = np.linalg.inv(T)\n",
    "    normalized_points = np.dot(T_inv, np.append(points, np.ones((points.shape[0], 1)), axis=1).T).T\n",
    "    return normalized_points, T\n",
    "\n",
    "def construct_matrix_A(points1, points2):\n",
    "    A = []\n",
    "    for i in range(points1.shape[0]):\n",
    "        x1, y1 = points1[i, 0], points1[i, 1]\n",
    "        x2, y2 = points2[i, 0], points2[i, 1]\n",
    "        A.append([-x1, -y1, -1, 0, 0, 0, x2*x1, x2*y1, x2])\n",
    "        A.append([0, 0, 0, -x1, -y1, -1, y2*x1, y2*y1, y2])\n",
    "    return np.array(A)\n",
    "\n",
    "def compute_homography(points1, points2):\n",
    "    points1_norm, T1 = normalize(points1)\n",
    "    points2_norm, T2 = normalize(points2)\n",
    "    A = construct_matrix_A(points1_norm, points2_norm)\n",
    "    _, _, V = np.linalg.svd(A)\n",
    "    H = V[-1].reshape(3, 3)\n",
    "    H = np.dot(np.linalg.inv(T2), np.dot(H, T1))\n",
    "    return H / H[2, 2]\n",
    "\n",
    "def normalize_points(points):\n",
    "    # Normalize points to have zero mean and unit variance\n",
    "    mean = np.mean(points, axis=0)\n",
    "    std = np.std(points, axis=0)\n",
    "    normalized_points = (points - mean) / std\n",
    "    return normalized_points, mean, std\n",
    "\n",
    "\n",
    "def denormalize_homography(H, mean_src, std_src, mean_dst, std_dst):\n",
    "    # Denormalize the homography matrix based on mean and standard deviation\n",
    "   # T_src = np.array([[1 / std_src[0], 0, -mean_src[0] / std_src[0]],\n",
    "            #          [0, 1 / std_src[1], -mean_src[1] / std_src[1]],\n",
    "            #          [0, 0, 1]])\n",
    "\n",
    "   # T_dst = np.array([[1 / std_dst[0], 0, -mean_dst[0] / std_dst[0]],\n",
    "   #                   [0, 1 / std_dst[1], -mean_dst[1] / std_dst[1]],\n",
    "   #                   [0, 0, 1]])\n",
    "    T_src = np.array([[std_src[0], 0, mean_src[0]], [0, std_src[1], mean_src[1]], [0, 0, 1]])\n",
    "    T_dst = np.array([[std_dst[0], 0, mean_dst[0]], [0, std_src[1], mean_dst[1]], [0, 0, 1]])\n",
    "\n",
    "    Homography = np.dot(np.linalg.inv(T_dst), np.dot(H, T_src))\n",
    "    \n",
    "    return  Homography\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "kp1 = kp_list[0]\n",
    "kp2 = kp_list[19]\n",
    "src_pts = np.float32([ kp1[q[0].queryIdx].pt for q in good ]).reshape(-1,1,2)\n",
    "dst_pts = np.float32([ kp2[t[0].trainIdx].pt for t in good ]).reshape(-1,1,2)\n",
    "src = np.reshape(src_pts,(np.shape(src_pts)[0],2))\n",
    "dst = np.reshape(dst_pts,(np.shape(dst_pts)[0],2))\n",
    "\n",
    "#src_pts_normalized, mean_src, std_src = normalize_points(src)\n",
    "#dst_pts_normalized, mean_dst, std_dst = normalize_points(dst)\n",
    "#src = preprocessing.normalize(src)   #Normalization\n",
    "#dst = preprocessing.normalize(dst)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Comp_H(src,dst):\n",
    "        A = []\n",
    "        for p, q in zip(src, dst):\n",
    "            x1 = p[0]\n",
    "            y1 = p[1]\n",
    "            x2 = q[0]\n",
    "            y2 = q[1]\n",
    "            A.append([-x1, -y1, -1, 0, 0, 0, x2*x1, x2*y1, x2])\n",
    "            A.append([0, 0, 0, -x1, -y1, -1, y2*x1, y2*y1, y2])\n",
    "\n",
    "        _, _, Vt = np.linalg.svd(A, full_matrices=True)\n",
    "        x = Vt[-1]\n",
    "        homography = x.reshape(3, -1) #/ x[-1]\n",
    "        return homography\n",
    "\n",
    "def RANSAC(Comp_H,src,dst,iter,threshold):\n",
    "      best_homography = None\n",
    "      inliers = [0]\n",
    "      for t in range(iter):\n",
    "            sample_indices = np.random.choice(int(len(src)), size=4, replace=False)\n",
    "            # Compute the Homography\n",
    "            H = Comp_H(src[sample_indices],dst[sample_indices])\n",
    "           # H = denormalize_homography(homography)\n",
    "            inl = 0\n",
    "            for p, q in zip(src, dst):\n",
    "                x1 = p[0]\n",
    "                y1 = p[1]\n",
    "                x2 = q[0]\n",
    "                y2 = q[1]\n",
    "            # Transform the point using the estimated homography\n",
    "                transformed_point = np.dot(H, np.array([x1, y1, 1]))\n",
    "\n",
    "            # Normalize the transformed point\n",
    "                transformed_point /= transformed_point[2]\n",
    "\n",
    "            # Calculate the Euclidean distance between the transformed point and the actual point\n",
    "                distance = np.linalg.norm(np.array([x2, y2, 1]) - transformed_point)\n",
    "                if distance < threshold:\n",
    "                   inl += 1\n",
    "            if inl > inliers[0]:\n",
    "                 best_homography = H\n",
    "                 inliers[0] = inl\n",
    "      return best_homography, inliers[0] \n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "condition: 30609.206718393925 inliers:  44\n"
     ]
    }
   ],
   "source": [
    "H, inliers = RANSAC(Comp_H,src,dst,143,0.5)\n",
    "print('condition:',np.linalg.cond(H), 'inliers: ', inliers )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26395.908156677036"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#src_pts = np.float32([ kp1[all_matches[0][i][0]].pt for i in range(len(all_matches[0])) ]).reshape(-1,1,2)\n",
    "#dst_pts = np.float32([ kp2[all_matches[0][i][1]].pt for i in range(len(all_matches[0])) ]).reshape(-1,1,2)\n",
    "#M2, mask = cv2.findHomography(src_pts, dst_pts, cv2.RANSAC,5.0)\n",
    "#np.linalg.cond(M2)\n",
    "\n",
    "#kp1 = kp_list[0]\n",
    "#kp2 = kp_list[1]\n",
    "#src_pts = np.float32([ kp1[q.queryIdx].pt for q in match[0] ]).reshape(-1,1,2)\n",
    "#dst_pts = np.float32([ kp2[t.trainIdx].pt for t in match[1] ]).reshape(-1,1,2)\n",
    "M, _ = cv2.findHomography(src, dst, cv2.RANSAC)\n",
    "np.linalg.cond(M)\n",
    "\n",
    "\n",
    "#picture2 = np.reshape(np.array([552,59,1]),(3,1))\n",
    "#picture1 = np.reshape(np.array([549,56,1]),(3,1))\n",
    "#pic_h = np.matmul(H,picture1)\n",
    "#print('the error is: \\n', pic_h/pic_h[2]-picture1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "homography = H\n",
    "\n",
    "def apply_homography(image, H):\n",
    "    \"\"\" Apply homography to the image \"\"\"\n",
    "    warped_img = cv2.warpPerspective(image, H, (image.shape[1], image.shape[0]))\n",
    "    return warped_img\n",
    "\n",
    "img_src = img1\n",
    "img_dest = img2\n",
    "\n",
    "warped_src = apply_homography(img_src, homography)\n",
    "#print('condition:',np.linalg.cond(H),'inliers: ', inliers)\n",
    "cv2.imshow('Warped Source Image', warped_src)\n",
    "cv2.imshow('Source Image', img_src)\n",
    "cv2.imshow('Destination Image', img_dest)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Testing Zone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "861174.1382601217"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getPerspectiveTransform(src, dst):\n",
    "    if len(src) == len(dst):\n",
    "        # Make homogeneous coordiates if necessary\n",
    "        if src.shape[1] == 2:\n",
    "            src = np.hstack((src, np.ones((len(src), 1), dtype=src.dtype)))\n",
    "        if dst.shape[1] == 2:\n",
    "            dst = np.hstack((dst, np.ones((len(dst), 1), dtype=dst.dtype)))\n",
    "\n",
    "        # Solve 'Ax = 0'\n",
    "        A = []\n",
    "        for p, q in zip(src, dst):\n",
    "            A.append([0, 0, 0, q[2]*p[0], q[2]*p[1], q[2]*p[2], -q[1]*p[0], -q[1]*p[1], -q[1]*p[2]])\n",
    "            A.append([q[2]*p[0], q[2]*p[1], q[2]*p[2], 0, 0, 0, -q[0]*p[0], -q[0]*p[1], -q[0]*p[2]])\n",
    "\n",
    "        eigenvalue,eigenvector=eig(np.matmul(np.transpose(A),A))\n",
    "        _, _, Vt = np.linalg.svd(A, full_matrices=True)\n",
    "        x = Vt[-1]\n",
    "         \n",
    "\n",
    "        # Reorganize `x` as a matrix\n",
    "        H = x.reshape(3, -1) / x[-1] # Normalize the last element as 1\n",
    "        return H\n",
    "    \n",
    "\n",
    "H_slides = getPerspectiveTransform(np.reshape(src_pts,(np.shape(src_pts)[0],2)), np.reshape(dst_pts,(np.shape(dst_pts)[0],2)))\n",
    "np.linalg.cond(H_slides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x = 422, y = 56\n",
      "x = 422, y = 56\n",
      "x = 422, y = 56\n",
      "x = 423, y = 56\n"
     ]
    }
   ],
   "source": [
    "def select_point(event,x,y,flags,param):\n",
    "    global ix,iy\n",
    "    if event == cv2.EVENT_LBUTTONDBLCLK: # captures left button double-click\n",
    "        print('x = %d, y = %d'%(x, y))\n",
    "\n",
    "cv2.namedWindow('frame')\n",
    "cv2.setMouseCallback('frame', select_point)\n",
    "cv2.imshow('frame',warped_src)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "capture = cv2.VideoCapture(os.path.abspath('trymefirst_lisbon.mp4'))\n",
    "framenr = 0 \n",
    "list_points = []\n",
    "while True:\n",
    "    success, frame = capture.read()\n",
    "    if success:\n",
    "        print('Current Frame!')\n",
    "        cv2.namedWindow('frame')\n",
    "        cv2.setMouseCallback('frame', select_point)\n",
    "        cv2.imshow('frame',frame)\n",
    "        if cv2.waitKey(0) & 0xFF == ord('q'):  ##press q if you want the video to stop \n",
    "             break\n",
    "        key = cv2.waitKey(0) & 0xFF == ord('k')\n",
    "        \n",
    "        print('New Frame!')\n",
    "\n",
    "capture.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
